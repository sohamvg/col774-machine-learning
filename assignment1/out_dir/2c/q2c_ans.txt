For values of r as 1, 100, and 10000, the stochastic gradient descent converged to nearly the same values which were close to the given theta. However, for r = 1000000, the algorithm seemed overshoot even for lower values of the learning rate. Also the algorithm converged within a few seconds for r=1, 100 but took around 5 minutes whereas r=1000000 didn't converge. It took around 9900 iterations in first epoch for r=1. It took around 300 iterations in first epoch for r=100. It took around 8 epochs and 800 iterations for r=10000 and r=1000000 didn't converge.  
