{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "table = str.maketrans('', '', string.punctuation) # punctuation table\n",
    "en_stop = set(stopwords.words('english')).union(set(string.punctuation)).union([\"n't\", \"'s\"]) # add punctuations and additional stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "p_stemmer = lru_cache(maxsize=None)(ps.stem)\n",
    "\n",
    "def json_writer(data, fname):\n",
    "    \"\"\"\n",
    "        Write multiple json files\n",
    "        Args:\n",
    "            data: list(dict): list of dictionaries to be written as json\n",
    "            fname: str: output file name\n",
    "    \"\"\"\n",
    "    with open(fname, mode=\"w\") as fp:\n",
    "        for line in data:\n",
    "            json.dump(line, fp)\n",
    "            fp.write(\"\\n\")\n",
    "\n",
    "\n",
    "def json_reader(fname):\n",
    "    \"\"\"\n",
    "        Read multiple json files\n",
    "        Args:\n",
    "            fname: str: input file\n",
    "        Returns:\n",
    "            generator: iterator over documents \n",
    "    \"\"\"\n",
    "    for line in open(fname, mode=\"r\"):\n",
    "        yield json.loads(line)\n",
    "\n",
    "\n",
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "\n",
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python.\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    en_stop = set(stopwords.words('english')).union(set(string.punctuation)).union([\"n't\", \"'s\"]) # add punctuations and additional stopwords\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    p_stemmer = lru_cache(maxsize=None)(ps.stem)\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        doc_count = 0\n",
    "        for item in docs:\n",
    "            doc_count += 1\n",
    "            if doc_count % 50000 == 0:\n",
    "                print(\"doc\", doc_count)\n",
    "            output_docs.append(_stem(item, p_stemmer, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, p_stemmer, en_stop, return_tokens)\n",
    "\n",
    "def indicator(exp):\n",
    "    if exp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[b'It is impo']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "new_text = \"It is important to by very pythonly Soham APPle apleLL while you are pythoning with python. ll pythoner,  s have pythoned poorly at least once. don't isn't shizdnt's but home's\"\n",
    "ss = np.empty((1), dtype='S10')\n",
    "ss[0] = new_text\n",
    "# print(getStemmedDocuments(new_text))\n",
    "print(ss)\n",
    "# print(getStemmedDocuments(ss))\n",
    "# word_tokenize(new_text)\n",
    "# p = set(string.punctuation)\n",
    "# print(p.union([\"n't\"]))\n",
    "# print(set(stopwords.words('english')).union(set(string.punctuation)).union([\"n't\", \"'s\"]))\n",
    "e,r = [],[]\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0 0\n",
      "train 50000 1208884\n",
      "train 100000 2004812\n",
      "train 150000 2672199\n",
      "train 200000 3264531\n",
      "train 250000 3809075\n",
      "train 300000 4319510\n",
      "train 350000 4794161\n",
      "train 400000 5245203\n",
      "train 450000 5681575\n",
      "train 500000 6100473\n",
      "test 0 6379340\n",
      "test 50000 6770362\n",
      "test 100000 7147753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_json = \"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\"\n",
    "test_json = \"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\"\n",
    "\n",
    "vocabulary = {}\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "i = -1\n",
    "for review in json_reader(train_json):\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    y.append(int(review[\"stars\"]))\n",
    "    \n",
    "    xi = []\n",
    "    words = _stem(review[\"text\"], p_stemmer, en_stop, return_tokens=True)\n",
    "    \n",
    "    # bi-grams\n",
    "    for w in range(len(words)-1):\n",
    "        bg = (words[w], words[w+1])\n",
    "        if bg not in vocabulary:\n",
    "            vocabulary[bg] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[bg])\n",
    "    x.append(xi)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "i = -1\n",
    "for review in json_reader(test_json):\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i, len(vocabulary))\n",
    "    y_test.append(int(review[\"stars\"]))\n",
    "    \n",
    "    xi = []\n",
    "    words = _stem(review[\"text\"], p_stemmer, en_stop, return_tokens=True)\n",
    "    \n",
    "    # bi-grams\n",
    "    for w in range(len(words)-1):\n",
    "        bg = (words[w], words[w+1])\n",
    "        if bg not in vocabulary:\n",
    "            vocabulary[bg] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[bg])\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "m_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0 0\n",
      "train 10000 341225\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-a5d4222aa49f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetStemmedDocuments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# bi-grams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-2d044ba41978>\u001b[0m in \u001b[0;36mgetStemmedDocuments\u001b[1;34m(docs, return_tokens)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mReference\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mpythonprogramming\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstemming\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtutorial\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0men_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0men_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n't\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add punctuations and \"n't\" to stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "# load training data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    m = sum(1 for line in train_file)\n",
    "    y = np.zeros(m, np.int)\n",
    "    x = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    i = -1\n",
    "    for line in train_file:\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(\"train\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        words = getStemmedDocuments(review[\"text\"])\n",
    "        \n",
    "        # bi-grams\n",
    "        for w in range(len(words)-1):\n",
    "            bg = (words[w], words[w+1])\n",
    "            if bg not in vocabulary:\n",
    "                vocabulary[bg] = len(vocabulary) + 1\n",
    "            xi.append(vocabulary[bg])\n",
    "        x.append(xi)\n",
    "\n",
    "# load test data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    m_test = sum(1 for line in test_file)\n",
    "    y_test = np.zeros(m_test, np.int)\n",
    "    x_test = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    i = -1\n",
    "    for line in test_file:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(\"test\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y_test[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        words = []\n",
    "        for word in getStemmedDocuments(review[\"text\"]):\n",
    "            word = word.translate(table) # remove punctuations\n",
    "            if word:\n",
    "                words.append(word)\n",
    "\n",
    "        # bi-grams\n",
    "        for i in range(len(words)-1):\n",
    "            bg = (words[i], words[i+1])\n",
    "            if bg not in vocabulary:\n",
    "                vocabulary[bg] = len(vocabulary) + 1\n",
    "            xi.append(vocabulary[bg])\n",
    "        x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5 # number of classes\n",
    "V = len(vocabulary)\n",
    "\n",
    "# y takes values in {1, 2, ..., r}; parameterized by phi\n",
    "# x takes values in {1, 2, ..., V}; parameterized by theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate phi\n",
    "phi = np.zeros(r)\n",
    "for i in range(m):\n",
    "    k = y[i] - 1\n",
    "    phi[k] += 1\n",
    "phi = phi / m\n",
    "\n",
    "# evaluate theta\n",
    "theta_numerator = np.zeros((V, r))\n",
    "theta_denominator = np.zeros((r))\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    ni = len(x[i])\n",
    "    k = y[i] - 1\n",
    "    for j in range(ni):\n",
    "        l = x[i][j] - 1\n",
    "        theta_numerator[l][k] += 1\n",
    "    theta_denominator[k] += ni\n",
    "    \n",
    "\n",
    "theta = np.zeros((V, r))\n",
    "for j in range(V):\n",
    "    for k in range(r):\n",
    "        theta[j][k] = (theta_numerator[j][k] + 1) / (theta_denominator[k] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9999999999999994"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "np.sum(theta[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, phi, theta):\n",
    "    \"\"\"\n",
    "        Returns: Most expected class label\n",
    "    \"\"\"\n",
    "    max_prob = -math.inf\n",
    "    argmax_k = -1\n",
    "    for k in range(r):\n",
    "        n = len(x)\n",
    "        summation = 0\n",
    "        for i in range(n):\n",
    "            l = x[i] - 1\n",
    "            summation += np.log(theta[l][k])\n",
    "        prob_k = summation + np.log(phi[k])\n",
    "        if prob_k > max_prob:\n",
    "            max_prob = prob_k\n",
    "            argmax_k = k\n",
    "    return argmax_k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0\n",
      "train 50000\n",
      "train 100000\n",
      "train 150000\n",
      "train 200000\n",
      "train 250000\n",
      "train 300000\n",
      "train 350000\n",
      "train 400000\n",
      "train 450000\n",
      "train 500000\n",
      "Train set accuracy 89.5231008540361\n"
     ]
    }
   ],
   "source": [
    "# train set accuracy\n",
    "\n",
    "train_count = 0\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i)\n",
    "    train_count += indicator(predict(x[i], phi, theta) == y[i])\n",
    "\n",
    "print(\"Train set accuracy\", (train_count / m) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test 0\n",
      "test 50000\n",
      "test 100000\n",
      "Test set accuracy 63.17997576990383\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "\n",
    "test_predictions = np.zeros(m_test, np.int)\n",
    "test_count = 0\n",
    "for i in range(m_test):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    test_predictions[i] = predict(x_test[i], phi, theta)\n",
    "    test_count += indicator(test_predictions[i] == y_test[i])\n",
    "\n",
    "print(\"Test set accuracy\", (test_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prediction and majority prediction\n",
    "\n",
    "majority_k = np.argmax(np.bincount(y)) # max occuring class\n",
    "\n",
    "random_count = 0\n",
    "majority_count = 0\n",
    "for i in range(len(y_test)):\n",
    "    random_count += indicator(y_test[i] == np.random.randint(r))\n",
    "    majority_count += indicator(y_test[i] == majority_k)\n",
    "\n",
    "print(\"Random prediction accuracy\", (random_count / m_test) * 100)\n",
    "print(\"Majority prediction accuracy\", (majority_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[16604  4130  1659   749  1162]\n [  739   780   222    70    74]\n [  659  1516  1730   454   181]\n [ 1327  3478  8863 16145  8181]\n [  840   934  2057 11940 49224]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((r, r), np.int)\n",
    "\n",
    "for i in range(m_test):\n",
    "    confusion_matrix[test_predictions[i]-1][y_test[i]-1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ]
}