{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "table = str.maketrans('', '', string.punctuation) # puctuation table\n",
    "\n",
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "\n",
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python.\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    p_stemmer = lru_cache(maxsize=None)(ps.stem) # use this function to stem\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        for item in docs:\n",
    "            output_docs.append(_stem(item, p_stemmer, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, p_stemmer, en_stop, return_tokens)\n",
    "\n",
    "def indicator(exp):\n",
    "    if exp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0 0\n",
      "train 50000 79624\n",
      "train 100000 119077\n",
      "train 150000 152151\n",
      "train 200000 181127\n",
      "train 250000 208287\n",
      "train 300000 233418\n",
      "train 350000 257386\n",
      "train 400000 280159\n",
      "train 450000 302198\n",
      "train 500000 323631\n",
      "test 0 337998\n",
      "test 50000 357942\n",
      "test 100000 377337\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "# load training data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    m = sum(1 for line in train_file)\n",
    "    y = np.zeros(m, np.int)\n",
    "    x = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    i = -1\n",
    "    for line in train_file:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(\"train\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        for word in review[\"text\"].split():\n",
    "            word = word.translate(table).lower()\n",
    "            if word:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(vocabulary) + 1\n",
    "                xi.append(vocabulary[word])\n",
    "        x.append(xi)\n",
    "\n",
    "# load test data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    m_test = sum(1 for line in test_file)\n",
    "    y_test = np.zeros(m_test, np.int)\n",
    "    x_test = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    i = -1\n",
    "    for line in test_file:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(\"test\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y_test[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        for word in review[\"text\"].split():\n",
    "            word = word.translate(table).lower()\n",
    "            if word:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(vocabulary) + 1\n",
    "                xi.append(vocabulary[word])\n",
    "        x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5 # number of classes\n",
    "V = len(vocabulary)\n",
    "\n",
    "# y takes values in {1, 2, ..., r}\n",
    "# x takes values in {1, 2, ..., V}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate phi\n",
    "phi = np.zeros(r)\n",
    "for i in range(m):\n",
    "    k = y[i] - 1\n",
    "    phi[k] += 1\n",
    "phi = phi / m\n",
    "\n",
    "# evaluate theta\n",
    "theta_numerator = np.zeros((V, r))\n",
    "theta_denominator = np.zeros((r))\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    ni = len(x[i])\n",
    "    k = y[i] - 1\n",
    "    for j in range(ni):\n",
    "        l = x[i][j] - 1\n",
    "        theta_numerator[l][k] += 1\n",
    "    theta_denominator[k] += ni\n",
    "    \n",
    "\n",
    "theta = np.zeros((V, r))\n",
    "for j in range(V):\n",
    "    for k in range(r):\n",
    "        theta[j][k] = (theta_numerator[j][k] + 1) / (theta_denominator[k] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, phi, theta):\n",
    "    \"\"\"\n",
    "        Returns: Most expected class label\n",
    "    \"\"\"\n",
    "    max_prob = -math.inf\n",
    "    argmax_k = -1\n",
    "    for k in range(r):\n",
    "        n = len(x)\n",
    "        summation = 0\n",
    "        for i in range(n):\n",
    "            l = x[i] - 1\n",
    "            summation += np.log(theta[l][k])\n",
    "        prob_k = summation + np.log(phi[k])\n",
    "        if prob_k > max_prob:\n",
    "            max_prob = prob_k\n",
    "            argmax_k = k\n",
    "    return argmax_k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0\n",
      "train 50000\n",
      "train 100000\n",
      "train 150000\n",
      "train 200000\n",
      "train 250000\n",
      "train 300000\n",
      "train 350000\n",
      "train 400000\n",
      "train 450000\n",
      "train 500000\n",
      "Train set accuracy 0.6463939035881482\n"
     ]
    }
   ],
   "source": [
    "# train set accuracy\n",
    "\n",
    "train_count = 0\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i)\n",
    "    train_count += indicator(predict(x[i], phi, theta) == y[i])\n",
    "\n",
    "print(\"Train set accuracy\", (train_count / m) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test 0\n",
      "test 50000\n",
      "test 100000\n",
      "Test set accuracy 60.525134985566645\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "\n",
    "test_predictions = np.zeros(m_test, np.int)\n",
    "test_count = 0\n",
    "for i in range(m_test):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    test_predictions[i] = predict(x_test[i], phi, theta)\n",
    "    test_count += indicator(test_predictions[i] == y_test[i])\n",
    "\n",
    "print(\"Test set accuracy\", (test_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "Random prediction accuracy 0.11215393589494309\n",
      "Majority prediction accuracy 0.439895900327555\n"
     ]
    }
   ],
   "source": [
    "# random prediction and majority prediction\n",
    "\n",
    "majority_k = np.argmax(np.bincount(y)) # max occuring class\n",
    "\n",
    "random_count = 0\n",
    "majority_count = 0\n",
    "for i in range(len(y_test)):\n",
    "    random_count += indicator(y_test[i] == np.random.randint(r))\n",
    "    majority_count += indicator(y_test[i] == majority_k)\n",
    "\n",
    "print(\"Random prediction accuracy\", (random_count / m_test) * 100)\n",
    "print(\"Majority prediction accuracy\", (majority_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[14751  3018  1461  1115  3046]\n [ 3173  2758  1316   490   215]\n [ 1204  3154  4468  1820   387]\n [  626  1470  6276 18523 14741]\n [  415   438  1010  7410 40433]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((r, r), np.int)\n",
    "\n",
    "for i in range(m_test):\n",
    "    confusion_matrix[test_predictions[i]-1][y_test[i]-1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ]
}