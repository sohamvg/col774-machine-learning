{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "digits_and_punctuation = str.maketrans('', '', string.digits + string.punctuation)\n",
    "\n",
    "def json_writer(data, fname):\n",
    "    \"\"\"\n",
    "        Write multiple json files\n",
    "        Args:\n",
    "            data: list(dict): list of dictionaries to be written as json\n",
    "            fname: str: output file name\n",
    "    \"\"\"\n",
    "    with open(fname, mode=\"w\") as fp:\n",
    "        for line in data:\n",
    "            json.dump(line, fp)\n",
    "            fp.write(\"\\n\")\n",
    "\n",
    "\n",
    "def json_reader(fname):\n",
    "    \"\"\"\n",
    "        Read multiple json files\n",
    "        Args:\n",
    "            fname: str: input file\n",
    "        Returns:\n",
    "            generator: iterator over documents \n",
    "    \"\"\"\n",
    "    for line in open(fname, mode=\"r\"):\n",
    "        yield json.loads(line)\n",
    "\n",
    "\n",
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "def _stem3(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in en_stop:\n",
    "            token = token.translate(digits_and_punctuation)\n",
    "            if token:\n",
    "                processed_tokens.append(token)\n",
    "\n",
    "    # stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), processed_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "\n",
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python.\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    # en_stop = set(stopwords.words('english')).union([\"n't\", \"'s\"]) # add additional stopwords\n",
    "\n",
    "    en_stop = set(stopwords.words('english')).union(set(string.punctuation)).union([\"n't\", \"'s\"]) # add punctuations and additional stopwords\n",
    "\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    p_stemmer = lru_cache(maxsize=None)(ps.stem)\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        doc_count = -1\n",
    "        for item in docs:\n",
    "            doc_count += 1\n",
    "            if doc_count % 50000 == 0:\n",
    "                print(\"doc\", doc_count)\n",
    "            output_docs.append(_stem(item, p_stemmer, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, p_stemmer, en_stop, return_tokens)\n",
    "\n",
    "def indicator(exp):\n",
    "    if exp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc 0\n",
      "doc 50000\n",
      "doc 100000\n",
      "doc 150000\n",
      "doc 200000\n",
      "doc 250000\n",
      "doc 300000\n",
      "doc 350000\n",
      "doc 400000\n",
      "doc 450000\n",
      "doc 500000\n",
      "train 0 0\n",
      "train 50000 49698\n",
      "train 100000 73107\n",
      "train 150000 92562\n",
      "train 200000 109577\n",
      "train 250000 125339\n",
      "train 300000 140057\n",
      "train 350000 153935\n",
      "train 400000 167009\n",
      "train 450000 179645\n",
      "train 500000 192098\n"
     ]
    }
   ],
   "source": [
    "#idf\n",
    "train_json = \"C:/IITD/sem5/col774-ml/datasets/col774_yelp_data/col774_yelp_data/train.json\"\n",
    "test_json = \"C:/IITD/sem5/col774-ml/datasets/col774_yelp_data/col774_yelp_data/test.json\"\n",
    "\n",
    "vocabulary = {}\n",
    "idf = {}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "docs = []\n",
    "\n",
    "for review in json_reader(train_json):\n",
    "    y.append(int(review[\"stars\"]))\n",
    "    docs.append(review[\"text\"])\n",
    "\n",
    "docs = getStemmedDocuments(docs)\n",
    "i = -1\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    unique_words_in_doc = set()\n",
    "    for word in doc:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "        if word not in unique_words_in_doc:\n",
    "            unique_words_in_doc.add(word)\n",
    "            if vocabulary[word] not in idf:\n",
    "                idf[vocabulary[word]] = 1\n",
    "            else:\n",
    "                idf[vocabulary[word]] += 1\n",
    "    x.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc 0\n",
      "doc 50000\n",
      "doc 100000\n",
      "doc 150000\n",
      "doc 200000\n",
      "doc 250000\n",
      "doc 300000\n",
      "doc 350000\n",
      "doc 400000\n",
      "doc 450000\n",
      "doc 500000\n",
      "train 0 0\n",
      "train 50000 1208884\n",
      "train 100000 2004812\n",
      "train 150000 2672199\n",
      "train 200000 3264531\n",
      "train 250000 3809075\n",
      "train 300000 4319510\n",
      "train 350000 4794161\n",
      "train 400000 5245203\n",
      "train 450000 5681575\n",
      "train 500000 6100473\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "train_json = \"C:/IITD/sem5/col774-ml/datasets/col774_yelp_data/col774_yelp_data/train.json\"\n",
    "test_json = \"C:/IITD/sem5/col774-ml/datasets/col774_yelp_data/col774_yelp_data/test.json\"\n",
    "\n",
    "vocabulary = {}\n",
    "idf = {}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "docs = []\n",
    "\n",
    "for review in json_reader(train_json):\n",
    "    y.append(int(review[\"stars\"]))\n",
    "    docs.append(review[\"text\"])\n",
    "\n",
    "docs = getStemmedDocuments(docs)\n",
    "i = -1\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    xi = []\n",
    "\n",
    "    for w in range(len(doc)-1):\n",
    "        bg = (doc[w], doc[w+1])\n",
    "        if bg not in vocabulary:\n",
    "            vocabulary[bg] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[bg])\n",
    "\n",
    "    x.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc 0\n",
      "doc 50000\n",
      "doc 100000\n",
      "test 0\n",
      "test 50000\n",
      "test 100000\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "docs_test = []\n",
    "\n",
    "for review in json_reader(test_json):\n",
    "    y_test.append(int(review[\"stars\"]))\n",
    "    docs_test.append(review[\"text\"])\n",
    "\n",
    "docs_test = getStemmedDocuments(docs_test)\n",
    "i = -1\n",
    "for doc in docs_test:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    xi = []\n",
    "    for word in doc:\n",
    "        if word in vocabulary:\n",
    "            xi.append(vocabulary[word])\n",
    "\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc 0\n",
      "doc 50000\n",
      "doc 100000\n",
      "test 0\n",
      "test 50000\n",
      "test 100000\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "docs_test = []\n",
    "\n",
    "for review in json_reader(test_json):\n",
    "    y_test.append(int(review[\"stars\"]))\n",
    "    docs_test.append(review[\"text\"])\n",
    "\n",
    "docs_test = getStemmedDocuments(docs_test)\n",
    "i = -1\n",
    "for doc in docs_test:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    xi = []\n",
    "    for w in range(len(doc)-1):\n",
    "        bg = (doc[w], doc[w+1])\n",
    "        if bg in vocabulary:\n",
    "            xi.append(vocabulary[bg])\n",
    "\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0 0\n",
      "train 50000 2512663\n",
      "train 100000 4835822\n",
      "train 150000 7046756\n",
      "train 200000 9187084\n",
      "train 250000 11277370\n",
      "train 300000 13323260\n",
      "train 350000 15317962\n",
      "train 400000 17277846\n",
      "train 450000 19225713\n",
      "train 500000 21142332\n",
      "test 0 22453011\n",
      "test 50000 24313219\n",
      "test 100000 26157342\n"
     ]
    }
   ],
   "source": [
    "# tri-grams\n",
    "vocabulary = {}\n",
    "x = []\n",
    "\n",
    "i = -1\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    for w in range(len(doc)-2):\n",
    "        word = (doc[w], doc[w+1], doc[w+2])\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "    x.append(xi)\n",
    "\n",
    "x_test = []\n",
    "\n",
    "i = -1\n",
    "for doc in docs_test:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    for w in range(len(doc)-2):\n",
    "        word = (doc[w], doc[w+1], doc[w+2])\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "m_test = len(y_test)\n",
    "\n",
    "# df = idf\n",
    "def _idf(word):\n",
    "    return np.log(m/(1 + df[word]))\n",
    "\n",
    "# print(_idf(1))\n",
    "\n",
    "# _idfs = np.array([_idf(word) for word in df.keys()])\n",
    "# print(len(vocabulary), len(df))\n",
    "# list(df.keys())\n",
    "# _idfs = _idfs / np.linalg.norm(_idfs)\n",
    "# print(np.min(_idfs), np.max(_idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sambusk\n"
     ]
    }
   ],
   "source": [
    "mini = np.argmin(_idfs) + 1\n",
    "maxi = np.argmax(_idfs) + 1\n",
    "\n",
    "for key, val in vocabulary.items():\n",
    "    if val == maxi:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6379340"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "r = 5 # number of classes\n",
    "V = len(vocabulary)\n",
    "\n",
    "# y takes values in {1, 2, ..., r}; parameterized by phi\n",
    "# x takes values in {1, 2, ..., V}; parameterized by theta\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "theta 0\n",
      "theta 500000\n",
      "theta 1000000\n",
      "theta 1500000\n",
      "theta 2000000\n",
      "theta 2500000\n",
      "theta 3000000\n",
      "theta 3500000\n",
      "theta 4000000\n",
      "theta 4500000\n",
      "theta 5000000\n",
      "theta 5500000\n",
      "theta 6000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate phi\n",
    "phi = np.zeros(r)\n",
    "for i in range(m):\n",
    "    k = y[i] - 1\n",
    "    phi[k] += 1\n",
    "phi = phi / m\n",
    "\n",
    "# evaluate theta\n",
    "theta_numerator = np.zeros((V, r))\n",
    "theta_denominator = np.zeros((r))\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    ni = len(x[i])\n",
    "    k = y[i] - 1\n",
    "    for j in range(ni):\n",
    "        l = x[i][j] - 1\n",
    "        theta_numerator[l][k] += 1\n",
    "    theta_denominator[k] += ni\n",
    "    \n",
    "\n",
    "theta = np.zeros((V, r))\n",
    "for j in range(V):\n",
    "    if j % 500000 == 0:\n",
    "        print(\"theta\", j)\n",
    "    for k in range(r):\n",
    "        theta[j][k] = (theta_numerator[j][k] + 1) / (theta_denominator[k] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.999999999999999"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "np.sum(theta[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, phi, theta):\n",
    "    \"\"\"\n",
    "        Returns: Most expected class label\n",
    "    \"\"\"\n",
    "    max_prob = -math.inf\n",
    "    argmax_k = -1\n",
    "    for k in range(r):\n",
    "        n = len(x)\n",
    "        summation = 0\n",
    "        for i in range(n):\n",
    "            l = x[i] - 1\n",
    "            summation += np.log(theta[l][k])\n",
    "        prob_k = summation + np.log(phi[k])\n",
    "        if prob_k > max_prob:\n",
    "            max_prob = prob_k\n",
    "            argmax_k = k\n",
    "    return argmax_k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test 0\n",
      "test 50000\n",
      "test 100000\n",
      "Test set accuracy 63.297387038394234\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "\n",
    "test_predictions = np.zeros(m_test, np.int)\n",
    "test_count = 0\n",
    "for i in range(m_test):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    test_predictions[i] = predict(x_test[i], phi, theta)    \n",
    "    test_count += indicator(test_predictions[i] == y_test[i])\n",
    "\n",
    "print(\"Test set accuracy\", (test_count / m_test) * 100)\n",
    "# Test set accuracy 63.10294799503433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prediction and majority prediction\n",
    "\n",
    "majority_k = np.argmax(np.bincount(y)) # max occuring class\n",
    "\n",
    "random_count = 0\n",
    "majority_count = 0\n",
    "for i in range(len(y_test)):\n",
    "    random_count += indicator(y_test[i] == np.random.randint(r))\n",
    "    majority_count += indicator(y_test[i] == majority_k)\n",
    "\n",
    "print(\"Random prediction accuracy\", (random_count / m_test) * 100)\n",
    "print(\"Majority prediction accuracy\", (majority_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[13494  2489  1215  1027  2954]\n [ 4860  4161  2352  1149   660]\n [  984  2843  5369  3507  1035]\n [  362   903  4490 16134 14421]\n [  469   442  1105  7541 39752]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((r, r), np.int)\n",
    "\n",
    "for i in range(m_test):\n",
    "    confusion_matrix[test_predictions[i]-1][y_test[i]-1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ]
}