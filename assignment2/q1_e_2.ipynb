{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "\n",
    "def json_writer(data, fname):\n",
    "    \"\"\"\n",
    "        Write multiple json files\n",
    "        Args:\n",
    "            data: list(dict): list of dictionaries to be written as json\n",
    "            fname: str: output file name\n",
    "    \"\"\"\n",
    "    with open(fname, mode=\"w\") as fp:\n",
    "        for line in data:\n",
    "            json.dump(line, fp)\n",
    "            fp.write(\"\\n\")\n",
    "\n",
    "\n",
    "def json_reader(fname):\n",
    "    \"\"\"\n",
    "        Read multiple json files\n",
    "        Args:\n",
    "            fname: str: input file\n",
    "        Returns:\n",
    "            generator: iterator over documents \n",
    "    \"\"\"\n",
    "    for line in open(fname, mode=\"r\"):\n",
    "        yield json.loads(line)\n",
    "\n",
    "\n",
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "\n",
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python.\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    en_stop = set(stopwords.words('english')).union(set(string.punctuation)).union([\"n't\", \"'s\"]) # add punctuations and additional stopwords\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    p_stemmer = lru_cache(maxsize=None)(ps.stem)\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        doc_count = 0\n",
    "        for item in docs:\n",
    "            doc_count += 1\n",
    "            if doc_count % 50000 == 0:\n",
    "                print(\"doc\", doc_count)\n",
    "            output_docs.append(_stem(item, p_stemmer, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, p_stemmer, en_stop, return_tokens)\n",
    "\n",
    "def indicator(exp):\n",
    "    if exp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc 50000\n",
      "doc 100000\n",
      "doc 150000\n",
      "doc 200000\n",
      "doc 250000\n",
      "doc 300000\n",
      "doc 350000\n",
      "doc 400000\n",
      "doc 450000\n",
      "doc 500000\n",
      "train 0 0\n",
      "train 10000 24029\n",
      "train 20000 34967\n",
      "train 30000 43637\n",
      "train 40000 51786\n",
      "train 50000 58935\n",
      "train 60000 65282\n",
      "train 70000 71380\n",
      "train 80000 77281\n",
      "train 90000 83119\n",
      "train 100000 88547\n",
      "train 110000 93837\n",
      "train 120000 98961\n",
      "train 130000 103761\n",
      "train 140000 108628\n",
      "train 150000 113215\n",
      "train 160000 117944\n",
      "train 170000 122418\n",
      "train 180000 126711\n",
      "train 190000 130906\n",
      "train 200000 135010\n",
      "train 210000 139158\n",
      "train 220000 143381\n",
      "train 230000 147253\n",
      "train 240000 151313\n",
      "train 250000 155213\n",
      "train 260000 158963\n",
      "train 270000 162770\n",
      "train 280000 166477\n",
      "train 290000 170316\n",
      "train 300000 174015\n",
      "train 310000 177741\n",
      "train 320000 181350\n",
      "train 330000 184821\n",
      "train 340000 188347\n",
      "train 350000 191810\n",
      "train 360000 195232\n",
      "train 370000 198658\n",
      "train 380000 202029\n",
      "train 390000 205378\n",
      "train 400000 208537\n",
      "train 410000 211909\n",
      "train 420000 215219\n",
      "train 430000 218616\n",
      "train 440000 221674\n",
      "train 450000 224829\n",
      "train 460000 228025\n",
      "train 470000 231222\n",
      "train 480000 234435\n",
      "train 490000 237566\n",
      "train 500000 240761\n",
      "train 510000 243896\n",
      "train 520000 246954\n",
      "train 530000 249875\n",
      "doc 50000\n",
      "doc 100000\n",
      "test 0 251412\n",
      "test 50000 266292\n",
      "test 100000 280405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_json = \"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\"\n",
    "test_json = \"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\"\n",
    "\n",
    "vocabulary = {}\n",
    "idf = {}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "docs = []\n",
    "\n",
    "for review in json_reader(train_json):\n",
    "    y.append(int(review[\"stars\"]))\n",
    "    docs.append(review[\"text\"])\n",
    "\n",
    "docs = getStemmedDocuments(docs)\n",
    "i = -1\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    unique_words_in_doc = set()\n",
    "    for word in doc:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "        if word not in unique_words_in_doc:\n",
    "            unique_words_in_doc.add(word)\n",
    "            if vocabulary[word] not in idf:\n",
    "                idf[vocabulary[word]] = 1\n",
    "            else:\n",
    "                idf[vocabulary[word]] += 1\n",
    "    x.append(xi)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "docs_test = []\n",
    "\n",
    "for review in json_reader(test_json):\n",
    "    y_test.append(int(review[\"stars\"]))\n",
    "    docs_test.append(review[\"text\"])\n",
    "\n",
    "docs_test = getStemmedDocuments(docs_test)\n",
    "i = -1\n",
    "for doc in docs_test:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    for word in doc:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0 0\n",
      "train 50000 2512663\n",
      "train 100000 4835822\n",
      "train 150000 7046756\n",
      "train 200000 9187084\n",
      "train 250000 11277370\n",
      "train 300000 13323260\n",
      "train 350000 15317962\n",
      "train 400000 17277846\n",
      "train 450000 19225713\n",
      "train 500000 21142332\n",
      "test 0 22453011\n",
      "test 50000 24313219\n",
      "test 100000 26157342\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "x = []\n",
    "\n",
    "i = -1\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    for w in range(len(doc)-2):\n",
    "        word = (doc[w], doc[w+1], doc[w+2])\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "    x.append(xi)\n",
    "\n",
    "x_test = []\n",
    "\n",
    "i = -1\n",
    "for doc in docs_test:\n",
    "    i += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i, len(vocabulary))\n",
    "    xi = []\n",
    "    for w in range(len(doc)-2):\n",
    "        word = (doc[w], doc[w+1], doc[w+2])\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary) + 1\n",
    "        xi.append(vocabulary[word])\n",
    "\n",
    "    x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "m_test = len(y_test)\n",
    "\n",
    "# df = idf\n",
    "def _idf(word):\n",
    "    return np.log(m/(1 + df[word]))\n",
    "\n",
    "# print(_idf(1))\n",
    "\n",
    "# _idfs = np.array([_idf(word) for word in df.keys()])\n",
    "# print(len(vocabulary), len(df))\n",
    "# list(df.keys())\n",
    "# _idfs = _idfs / np.linalg.norm(_idfs)\n",
    "# print(np.min(_idfs), np.max(_idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sambusk\n"
     ]
    }
   ],
   "source": [
    "mini = np.argmin(_idfs) + 1\n",
    "maxi = np.argmax(_idfs) + 1\n",
    "\n",
    "for key, val in vocabulary.items():\n",
    "    if val == maxi:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5 # number of classes\n",
    "V = len(vocabulary)\n",
    "\n",
    "# y takes values in {1, 2, ..., r}; parameterized by phi\n",
    "# x takes values in {1, 2, ..., V}; parameterized by theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "theta 0\n",
      "theta 500000\n",
      "theta 1000000\n",
      "theta 1500000\n",
      "theta 2000000\n",
      "theta 2500000\n",
      "theta 3000000\n",
      "theta 3500000\n",
      "theta 4000000\n",
      "theta 4500000\n",
      "theta 5000000\n",
      "theta 5500000\n",
      "theta 6000000\n",
      "theta 6500000\n",
      "theta 7000000\n",
      "theta 7500000\n",
      "theta 8000000\n",
      "theta 8500000\n",
      "theta 9000000\n",
      "theta 9500000\n",
      "theta 10000000\n",
      "theta 10500000\n",
      "theta 11000000\n",
      "theta 11500000\n",
      "theta 12000000\n",
      "theta 12500000\n",
      "theta 13000000\n",
      "theta 13500000\n",
      "theta 14000000\n",
      "theta 14500000\n",
      "theta 15000000\n",
      "theta 15500000\n",
      "theta 16000000\n",
      "theta 16500000\n",
      "theta 17000000\n",
      "theta 17500000\n",
      "theta 18000000\n",
      "theta 18500000\n",
      "theta 19000000\n",
      "theta 19500000\n",
      "theta 20000000\n",
      "theta 20500000\n",
      "theta 21000000\n",
      "theta 21500000\n",
      "theta 22000000\n",
      "theta 22500000\n",
      "theta 23000000\n",
      "theta 23500000\n",
      "theta 24000000\n",
      "theta 24500000\n",
      "theta 25000000\n",
      "theta 25500000\n",
      "theta 26000000\n",
      "theta 26500000\n",
      "theta 27000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate phi\n",
    "phi = np.zeros(r)\n",
    "for i in range(m):\n",
    "    k = y[i] - 1\n",
    "    phi[k] += 1\n",
    "phi = phi / m\n",
    "\n",
    "# evaluate theta\n",
    "theta_numerator = np.zeros((V, r))\n",
    "theta_denominator = np.zeros((r))\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    ni = len(x[i])\n",
    "    k = y[i] - 1\n",
    "    for j in range(ni):\n",
    "        l = x[i][j] - 1\n",
    "        theta_numerator[l][k] += 1\n",
    "    theta_denominator[k] += ni\n",
    "    \n",
    "\n",
    "theta = np.zeros((V, r))\n",
    "for j in range(V):\n",
    "    if j % 500000 == 0:\n",
    "        print(\"theta\", j)\n",
    "    for k in range(r):\n",
    "        theta[j][k] = (theta_numerator[j][k] + 1) / (theta_denominator[k] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0000000000000344"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "np.sum(theta[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, phi, theta):\n",
    "    \"\"\"\n",
    "        Returns: Most expected class label\n",
    "    \"\"\"\n",
    "    max_prob = -math.inf\n",
    "    argmax_k = -1\n",
    "    for k in range(r):\n",
    "        n = len(x)\n",
    "        summation = 0\n",
    "        for i in range(n):\n",
    "            l = x[i] - 1\n",
    "            summation += np.log(theta[l][k])\n",
    "        prob_k = summation + np.log(phi[k])\n",
    "        if prob_k > max_prob:\n",
    "            max_prob = prob_k\n",
    "            argmax_k = k\n",
    "    return argmax_k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0\n",
      "train 50000\n",
      "train 100000\n",
      "train 150000\n",
      "train 200000\n",
      "train 250000\n",
      "train 300000\n",
      "train 350000\n",
      "train 400000\n",
      "train 450000\n",
      "train 500000\n",
      "Train set accuracy 66.72512302008705\n"
     ]
    }
   ],
   "source": [
    "# train set accuracy\n",
    "\n",
    "train_count = 0\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i)\n",
    "    train_count += indicator(predict(x[i], phi, theta) == y[i])\n",
    "\n",
    "print(\"Train set accuracy\", (train_count / m) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test 0\n",
      "test 50000\n",
      "test 100000\n",
      "Test set accuracy 55.987226850536196\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "\n",
    "test_predictions = np.zeros(m_test, np.int)\n",
    "test_count = 0\n",
    "for i in range(m_test):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    test_predictions[i] = predict(x_test[i], phi, theta)\n",
    "    test_count += indicator(test_predictions[i] == y_test[i])\n",
    "\n",
    "print(\"Test set accuracy\", (test_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prediction and majority prediction\n",
    "\n",
    "majority_k = np.argmax(np.bincount(y)) # max occuring class\n",
    "\n",
    "random_count = 0\n",
    "majority_count = 0\n",
    "for i in range(len(y_test)):\n",
    "    random_count += indicator(y_test[i] == np.random.randint(r))\n",
    "    majority_count += indicator(y_test[i] == majority_k)\n",
    "\n",
    "print(\"Random prediction accuracy\", (random_count / m_test) * 100)\n",
    "print(\"Majority prediction accuracy\", (majority_count / m_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[13494  2489  1215  1027  2954]\n [ 4860  4161  2352  1149   660]\n [  984  2843  5369  3507  1035]\n [  362   903  4490 16134 14421]\n [  469   442  1105  7541 39752]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((r, r), np.int)\n",
    "\n",
    "for i in range(m_test):\n",
    "    confusion_matrix[test_predictions[i]-1][y_test[i]-1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ]
}