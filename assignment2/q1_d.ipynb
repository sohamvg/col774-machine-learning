{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "table = str.maketrans('', '', string.punctuation) # puctuation table\n",
    "\n",
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)\n",
    "\n",
    "\n",
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python.\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    p_stemmer = lru_cache(maxsize=None)(ps.stem) # use this function to stem\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        for item in docs:\n",
    "            output_docs.append(_stem(item, p_stemmer, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, p_stemmer, en_stop, return_tokens)\n",
    "\n",
    "def indicator(exp):\n",
    "    if exp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythonly Soham APPle apleLL while you are pythoning with python. ll pythoners have pythoned poorly at least once.\"\n",
    "print(getStemmedDocuments(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "# load training data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    m = sum(1 for line in train_file)\n",
    "    y = np.zeros(m, np.int)\n",
    "    x = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/train.json\") as train_file:\n",
    "    i = -1\n",
    "    for line in train_file:\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(\"train\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        for word in getStemmedDocuments(review[\"text\"]):\n",
    "            word = word.translate(table)\n",
    "            if word:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(vocabulary) + 1\n",
    "                xi.append(vocabulary[word])\n",
    "        x.append(xi)\n",
    "\n",
    "# load test data\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    m_test = sum(1 for line in test_file)\n",
    "    y_test = np.zeros(m_test, np.int)\n",
    "    x_test = []\n",
    "\n",
    "with open(\"C:/IITD/sem5/col774-ml/col774_yelp_data/col774_yelp_data/test.json\") as test_file:\n",
    "    i = -1\n",
    "    for line in test_file:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(\"test\", i, len(vocabulary))\n",
    "        review = json.loads(line)\n",
    "        y_test[i] = int(review[\"stars\"])\n",
    "        \n",
    "        xi = []\n",
    "        for word in getStemmedDocuments(review[\"text\"]):\n",
    "            word = word.translate(table)\n",
    "            if word:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(vocabulary) + 1\n",
    "                xi.append(vocabulary[word])\n",
    "        x_test.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5 # number of classes\n",
    "V = len(vocabulary)\n",
    "\n",
    "# y takes values in {1, 2, ..., r}; parameterized by phi\n",
    "# x takes values in {1, 2, ..., V}; parameterized by theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate phi\n",
    "phi = np.zeros(r)\n",
    "for i in range(m):\n",
    "    k = y[i] - 1\n",
    "    phi[k] += 1\n",
    "phi = phi / m\n",
    "\n",
    "# evaluate theta\n",
    "theta_numerator = np.zeros((V, r))\n",
    "theta_denominator = np.zeros((r))\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    ni = len(x[i])\n",
    "    k = y[i] - 1\n",
    "    for j in range(ni):\n",
    "        l = x[i][j] - 1\n",
    "        theta_numerator[l][k] += 1\n",
    "    theta_denominator[k] += ni\n",
    "    \n",
    "\n",
    "theta = np.zeros((V, r))\n",
    "for j in range(V):\n",
    "    for k in range(r):\n",
    "        theta[j][k] = (theta_numerator[j][k] + 1) / (theta_denominator[k] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, phi, theta):\n",
    "    \"\"\"\n",
    "        Returns: Most expected class label\n",
    "    \"\"\"\n",
    "    max_prob = -math.inf\n",
    "    argmax_k = -1\n",
    "    for k in range(r):\n",
    "        n = len(x)\n",
    "        summation = 0\n",
    "        for i in range(n):\n",
    "            l = x[i] - 1\n",
    "            summation += np.log(theta[l][k])\n",
    "        prob_k = summation + np.log(phi[k])\n",
    "        if prob_k > max_prob:\n",
    "            max_prob = prob_k\n",
    "            argmax_k = k\n",
    "    return argmax_k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set accuracy\n",
    "\n",
    "train_count = 0\n",
    "for i in range(m):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"train\", i)\n",
    "    train_count += indicator(predict(x[i], phi, theta) == y[i])\n",
    "\n",
    "print(\"Train set accuracy\", train_count / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set accuracy\n",
    "\n",
    "test_predictions = np.zeros(m_test, np.int)\n",
    "test_count = 0\n",
    "for i in range(m_test):\n",
    "    if i % 50000 == 0:\n",
    "        print(\"test\", i)\n",
    "    test_predictions[i] = predict(x_test[i], phi, theta)\n",
    "    test_count += indicator(test_predictions[i] == y_test[i])\n",
    "\n",
    "print(\"Test set accuracy\", test_count / m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prediction and majority prediction\n",
    "\n",
    "majority_k = np.argmax(np.bincount(y)) # max occuring class\n",
    "\n",
    "random_count = 0\n",
    "majority_count = 0\n",
    "for i in range(len(y_test)):\n",
    "    random_count += indicator(y_test[i] == np.random.randint(r))\n",
    "    majority_count += indicator(y_test[i] == majority_k)\n",
    "\n",
    "print(\"Random prediction accuracy\", random_count / m_test)\n",
    "print(\"Majority prediction accuracy\", majority_count / m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = np.zeros((r, r), np.int)\n",
    "\n",
    "for i in range(m_test):\n",
    "    confusion_matrix[test_predictions[i]-1][y_test[i]-1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ]
}