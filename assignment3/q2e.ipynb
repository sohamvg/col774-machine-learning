{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100), activation=\"relu\", solver='sgd', learning_rate=\"adaptive\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 1.85089384\n",
      "Iteration 2, loss = 0.74011113\n",
      "Iteration 3, loss = 0.39147671\n",
      "Iteration 4, loss = 0.29026448\n",
      "Iteration 5, loss = 0.24268176\n",
      "Iteration 6, loss = 0.21487770\n",
      "Iteration 7, loss = 0.19638462\n",
      "Iteration 8, loss = 0.18301464\n",
      "Iteration 9, loss = 0.17266060\n",
      "Iteration 10, loss = 0.16446662\n",
      "Iteration 11, loss = 0.15761881\n",
      "Iteration 12, loss = 0.15185713\n",
      "Iteration 13, loss = 0.14684217\n",
      "Iteration 14, loss = 0.14243538\n",
      "Iteration 15, loss = 0.13858405\n",
      "Iteration 16, loss = 0.13493309\n",
      "Iteration 17, loss = 0.13171555\n",
      "Iteration 18, loss = 0.12874558\n",
      "Iteration 19, loss = 0.12601816\n",
      "Iteration 20, loss = 0.12349464\n",
      "Iteration 21, loss = 0.12112359\n",
      "Iteration 22, loss = 0.11890815\n",
      "Iteration 23, loss = 0.11684910\n",
      "Iteration 24, loss = 0.11476165\n",
      "Iteration 25, loss = 0.11289379\n",
      "Iteration 26, loss = 0.11113942\n",
      "Iteration 27, loss = 0.10935906\n",
      "Iteration 28, loss = 0.10776611\n",
      "Iteration 29, loss = 0.10619115\n",
      "Iteration 30, loss = 0.10463981\n",
      "Iteration 31, loss = 0.10326779\n",
      "Iteration 32, loss = 0.10186498\n",
      "Iteration 33, loss = 0.10052899\n",
      "Iteration 34, loss = 0.09920538\n",
      "Iteration 35, loss = 0.09791096\n",
      "Iteration 36, loss = 0.09665291\n",
      "Iteration 37, loss = 0.09556215\n",
      "Iteration 38, loss = 0.09436052\n",
      "Iteration 39, loss = 0.09325275\n",
      "Iteration 40, loss = 0.09214229\n",
      "Iteration 41, loss = 0.09115866\n",
      "Iteration 42, loss = 0.09005062\n",
      "Iteration 43, loss = 0.08909540\n",
      "Iteration 44, loss = 0.08806416\n",
      "Iteration 45, loss = 0.08701374\n",
      "Iteration 46, loss = 0.08613883\n",
      "Iteration 47, loss = 0.08516294\n",
      "Iteration 48, loss = 0.08421117\n",
      "Iteration 49, loss = 0.08341600\n",
      "Iteration 50, loss = 0.08249915\n",
      "Iteration 51, loss = 0.08168646\n",
      "Iteration 52, loss = 0.08080264\n",
      "Iteration 53, loss = 0.07994417\n",
      "Iteration 54, loss = 0.07923209\n",
      "Iteration 55, loss = 0.07842379\n",
      "Iteration 56, loss = 0.07763645\n",
      "Iteration 57, loss = 0.07688504\n",
      "Iteration 58, loss = 0.07614345\n",
      "Iteration 59, loss = 0.07541107\n",
      "Iteration 60, loss = 0.07464323\n",
      "Iteration 61, loss = 0.07398970\n",
      "Iteration 62, loss = 0.07329982\n",
      "Iteration 63, loss = 0.07264474\n",
      "Iteration 64, loss = 0.07189460\n",
      "Iteration 65, loss = 0.07126509\n",
      "Iteration 66, loss = 0.07059891\n",
      "Iteration 67, loss = 0.06993287\n",
      "Iteration 68, loss = 0.06932827\n",
      "Iteration 69, loss = 0.06868816\n",
      "Iteration 70, loss = 0.06806049\n",
      "Iteration 71, loss = 0.06754406\n",
      "Iteration 72, loss = 0.06690305\n",
      "Iteration 73, loss = 0.06627446\n",
      "Iteration 74, loss = 0.06566515\n",
      "Iteration 75, loss = 0.06511018\n",
      "Iteration 76, loss = 0.06458452\n",
      "Iteration 77, loss = 0.06397159\n",
      "Iteration 78, loss = 0.06337947\n",
      "Iteration 79, loss = 0.06288798\n",
      "Iteration 80, loss = 0.06233362\n",
      "Iteration 81, loss = 0.06188667\n",
      "Iteration 82, loss = 0.06130296\n",
      "Iteration 83, loss = 0.06087207\n",
      "Iteration 84, loss = 0.06029906\n",
      "Iteration 85, loss = 0.05985280\n",
      "Iteration 86, loss = 0.05929729\n",
      "Iteration 87, loss = 0.05884678\n",
      "Iteration 88, loss = 0.05835654\n",
      "Iteration 89, loss = 0.05791257\n",
      "Iteration 90, loss = 0.05742772\n",
      "Iteration 91, loss = 0.05689703\n",
      "Iteration 92, loss = 0.05650672\n",
      "Iteration 93, loss = 0.05603683\n",
      "Iteration 94, loss = 0.05549466\n",
      "Iteration 95, loss = 0.05510055\n",
      "Iteration 96, loss = 0.05468671\n",
      "Iteration 97, loss = 0.05433422\n",
      "Iteration 98, loss = 0.05387272\n",
      "Iteration 99, loss = 0.05343898\n",
      "Iteration 100, loss = 0.05305940\n",
      "Iteration 101, loss = 0.05265316\n",
      "Iteration 102, loss = 0.05219167\n",
      "Iteration 103, loss = 0.05184184\n",
      "Iteration 104, loss = 0.05140687\n",
      "Iteration 105, loss = 0.05102414\n",
      "Iteration 106, loss = 0.05060929\n",
      "Iteration 107, loss = 0.05016904\n",
      "Iteration 108, loss = 0.04981119\n",
      "Iteration 109, loss = 0.04951369\n",
      "Iteration 110, loss = 0.04904280\n",
      "Iteration 111, loss = 0.04878394\n",
      "Iteration 112, loss = 0.04834023\n",
      "Iteration 113, loss = 0.04806931\n",
      "Iteration 114, loss = 0.04765351\n",
      "Iteration 115, loss = 0.04731635\n",
      "Iteration 116, loss = 0.04696337\n",
      "Iteration 117, loss = 0.04665185\n",
      "Iteration 118, loss = 0.04627424\n",
      "Iteration 119, loss = 0.04594996\n",
      "Iteration 120, loss = 0.04564871\n",
      "Iteration 121, loss = 0.04531786\n",
      "Iteration 122, loss = 0.04496789\n",
      "Iteration 123, loss = 0.04471489\n",
      "Iteration 124, loss = 0.04431737\n",
      "Iteration 125, loss = 0.04401286\n",
      "Iteration 126, loss = 0.04370771\n",
      "Iteration 127, loss = 0.04337230\n",
      "Iteration 128, loss = 0.04317432\n",
      "Iteration 129, loss = 0.04282526\n",
      "Iteration 130, loss = 0.04248506\n",
      "Iteration 131, loss = 0.04224484\n",
      "Iteration 132, loss = 0.04196916\n",
      "Iteration 133, loss = 0.04166460\n",
      "Iteration 134, loss = 0.04135891\n",
      "Iteration 135, loss = 0.04107501\n",
      "Iteration 136, loss = 0.04076356\n",
      "Iteration 137, loss = 0.04054441\n",
      "Iteration 138, loss = 0.04024151\n",
      "Iteration 139, loss = 0.03994601\n",
      "Iteration 140, loss = 0.03969025\n",
      "Iteration 141, loss = 0.03942636\n",
      "Iteration 142, loss = 0.03919037\n",
      "Iteration 143, loss = 0.03889202\n",
      "Iteration 144, loss = 0.03866058\n",
      "Iteration 145, loss = 0.03842229\n",
      "Iteration 146, loss = 0.03812396\n",
      "Iteration 147, loss = 0.03790320\n",
      "Iteration 148, loss = 0.03766060\n",
      "Iteration 149, loss = 0.03737476\n",
      "Iteration 150, loss = 0.03718393\n",
      "Iteration 151, loss = 0.03687456\n",
      "Iteration 152, loss = 0.03668543\n",
      "Iteration 153, loss = 0.03641578\n",
      "Iteration 154, loss = 0.03619510\n",
      "Iteration 155, loss = 0.03594763\n",
      "Iteration 156, loss = 0.03570810\n",
      "Iteration 157, loss = 0.03548219\n",
      "Iteration 158, loss = 0.03526191\n",
      "Iteration 159, loss = 0.03508039\n",
      "Iteration 160, loss = 0.03472345\n",
      "Iteration 161, loss = 0.03457482\n",
      "Iteration 162, loss = 0.03435874\n",
      "Iteration 163, loss = 0.03413290\n",
      "Iteration 164, loss = 0.03394904\n",
      "Iteration 165, loss = 0.03376577\n",
      "Iteration 166, loss = 0.03353545\n",
      "Iteration 167, loss = 0.03327413\n",
      "Iteration 168, loss = 0.03304053\n",
      "Iteration 169, loss = 0.03285543\n",
      "Iteration 170, loss = 0.03265561\n",
      "Iteration 171, loss = 0.03246596\n",
      "Iteration 172, loss = 0.03224254\n",
      "Iteration 173, loss = 0.03211961\n",
      "Iteration 174, loss = 0.03184698\n",
      "Iteration 175, loss = 0.03167659\n",
      "Iteration 176, loss = 0.03146213\n",
      "Iteration 177, loss = 0.03127380\n",
      "Iteration 178, loss = 0.03108748\n",
      "Iteration 179, loss = 0.03093153\n",
      "Iteration 180, loss = 0.03072441\n",
      "Iteration 181, loss = 0.03051741\n",
      "Iteration 182, loss = 0.03031513\n",
      "Iteration 183, loss = 0.03020981\n",
      "Iteration 184, loss = 0.02995969\n",
      "Iteration 185, loss = 0.02974187\n",
      "Iteration 186, loss = 0.02959623\n",
      "Iteration 187, loss = 0.02944553\n",
      "Iteration 188, loss = 0.02926930\n",
      "Iteration 189, loss = 0.02906323\n",
      "Iteration 190, loss = 0.02891403\n",
      "Iteration 191, loss = 0.02872891\n",
      "Iteration 192, loss = 0.02859418\n",
      "Iteration 193, loss = 0.02843100\n",
      "Iteration 194, loss = 0.02822375\n",
      "Iteration 195, loss = 0.02804574\n",
      "Iteration 196, loss = 0.02786697\n",
      "Iteration 197, loss = 0.02771476\n",
      "Iteration 198, loss = 0.02754238\n",
      "Iteration 199, loss = 0.02739355\n",
      "Iteration 200, loss = 0.02720874\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 100), learning_rate='adaptive',\n",
       "              solver='sgd', verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9296"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  }
 ]
}