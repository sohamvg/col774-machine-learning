{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    ohe = np.zeros((y.size, y.max()+1))\n",
    "    ohe[np.arange(y.size), y] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "hidden_layers = [15]\n",
    "r = 10\n",
    "layers = hidden_layers.copy()\n",
    "layers.append(r)\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "y_train = one_hot_encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "        sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netj(theta_j, x_j):\n",
    "    return np.dot(theta_j.T, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_x0(x):\n",
    "    return x\n",
    "    # return np.hstack((np.ones(1), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    o = [np.zeros(l) for l in layers]\n",
    "\n",
    "    for l in range(len(layers)):\n",
    "        for j in range(layers[l]):\n",
    "            if l == 0:\n",
    "                o[l][j] = g(netj(theta[l][j], add_x0(x)))\n",
    "            else:\n",
    "                o[l][j] = g(netj(theta[l][j], add_x0(o[l-1]))) # use all outputs of prev layer as network is fully connected\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, o, theta):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            y: class labels\n",
    "            o: outputs of each layer\n",
    "            theta: parameters\n",
    "        Returns:\n",
    "            deltas: deltas[l][j] for each layer l and perceptron j\n",
    "    \"\"\"\n",
    "    deltas = [np.zeros(l) for l in layers]\n",
    "\n",
    "    # output layer\n",
    "    output_layer = -1\n",
    "    delta = (y - o[output_layer]) * o[output_layer] * (1 - o[output_layer])\n",
    "    deltas[output_layer] = delta\n",
    "\n",
    "    # hidden layers\n",
    "    for l in reversed(range(len(hidden_layers))):\n",
    "        for j in range(hidden_layers[l]):\n",
    "            deltas[l][j] = sum(deltas[l+1][dwn_nbr] * theta[l+1][dwn_nbr, j] * o[l][j] * (1 - o[l][j]) for dwn_nbr in range(layers[l+1]))\n",
    "        \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(theta, X, Y):\n",
    "    m = X.shape[0]\n",
    "    error = 0\n",
    "    for i in range(m):\n",
    "        x, y = X[i], Y[i]\n",
    "        o = forward_propagation(x, theta)\n",
    "        error += np.sum((y - o[-1]) ** 2)\n",
    "    return error / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, M, learning_rate, epsilon, max_epochs):\n",
    "    \"\"\"\n",
    "        mini-batch SGD\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    t = 0\n",
    "    k_repeats = 0\n",
    "    # theta = [np.random.rand(layers[0], n+1)] + [np.random.rand(layers[l], layers[l-1]+1) for l in range(1, len(layers))]\n",
    "    theta = [np.random.randn(layers[0], n) * np.sqrt(2/(n))] + [np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/layers[l-1]) for l in range(1, len(layers))]\n",
    "    # theta = [np.random.randn(layers[0], n+1) * np.sqrt(2/(n+1))] + [np.random.randn(layers[l], layers[l-1]+1) * np.sqrt(2/layers[l-1]+1) for l in range(1, len(layers))]\n",
    "    # prev_theta = copy.deepcopy(theta)\n",
    "    prev_cost = np.inf\n",
    "    # prev_cost = total_cost(theta, X_train, y_train)\n",
    "\n",
    "    # theta = [np.zeros((layers[0], n+1))] + [np.zeros((layers[l], layers[l-1]+1)) for l in range(1, len(layers))]\n",
    "    # print(\"theta\", theta)\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        if epoch > max_epochs:\n",
    "            return theta\n",
    "\n",
    "        print(\"epoch\", epoch, total_cost(theta, X_train, y_train))\n",
    "\n",
    "        # shuffle\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_e = X_train[indices]\n",
    "        y_train_e = y_train[indices]\n",
    "\n",
    "        for b in range(int(m/M)):\n",
    "            t += 1\n",
    "            # print(\"b\", b, epoch, t)\n",
    "            sum_J_theta_derivatives = [np.zeros((layers[0], n))] + [np.zeros((layers[l], layers[l-1])) for l in range(1, len(layers))]\n",
    "\n",
    "            for i in range(b * M, (b+1) * M):\n",
    "                x, y = X_train_e[i], y_train_e[i]\n",
    "                o = forward_propagation(x, theta)\n",
    "                deltas = back_propagation(y, o, theta)\n",
    "\n",
    "                # print(\"c\", y, o[-1], y - o[-1])\n",
    "                # cost += np.sum((y - o[-1]) ** 2)\n",
    "                # print(\"x\", x.shape, x)\n",
    "                # print(\"o\", o)\n",
    "                # print(\"deltas\", deltas)\n",
    "\n",
    "                # calculate J(theta) derivatives\n",
    "                for l in range(len(layers)):\n",
    "                    if l == 0:\n",
    "                        # x_j = add_x0(x)\n",
    "                        x_j = x\n",
    "                    else:\n",
    "                        # x_j = add_x0(o[l-1])\n",
    "                        x_j = o[l-1]\n",
    "                    for j in range(layers[l]):\n",
    "                        J_theta_derivative = - deltas[l][j] * x_j\n",
    "                        sum_J_theta_derivatives[l][j] += J_theta_derivative\n",
    "\n",
    "            # prev_theta = copy.deepcopy(theta)\n",
    "            # print(\"sumJ\", sum_J_theta_derivatives)\n",
    "            # print(\"err\", total_cost(theta, X_train, y_train))\n",
    "\n",
    "            # calculating cost over the examples seen in the lastest batch before updating theta\n",
    "            cost = total_cost(theta, X_train_e[b * M: (b+1) * M], y_train_e[b * M: (b+1) * M])\n",
    "            # print(\"cost\", prev_cost, cost, abs(prev_cost - cost))\n",
    "            if abs(prev_cost - cost) <= epsilon:\n",
    "                k_repeats += 1\n",
    "            else:\n",
    "                k_repeats = 0\n",
    "\n",
    "            if k_repeats > 2:\n",
    "                print(\"converged\")\n",
    "                return theta\n",
    "            prev_cost = cost\n",
    "\n",
    "            # update theta\n",
    "            for l in range(len(layers)):\n",
    "                for j in range(layers[l]):\n",
    "                    theta[l][j] = theta[l][j] - learning_rate * (sum_J_theta_derivatives[l][j] / M)\n",
    "\n",
    "\n",
    "\n",
    "            # print(\"w\", theta)\n",
    "\n",
    "            # break when stopping criteria meets\n",
    "            # converged = True\n",
    "            # theta_diff_all = 0\n",
    "            # for l in range(len(layers)):\n",
    "            #     # print(\"theta\", theta[l], prev_theta[l])\n",
    "            #     # print(\"theta_diff\", abs(theta[l] - prev_theta[l]))\n",
    "            #     theta_diff_all += np.sum(abs(theta[l] - prev_theta[l]))\n",
    "            #     if not (abs(theta[l] - prev_theta[l]) <= epsilon).all():\n",
    "            #         converged = False\n",
    "            #         break\n",
    "            # # print(\"stop\", coverged, theta_diff_all)\n",
    "            # if converged:\n",
    "            #     k_repeats += 1\n",
    "            # else:\n",
    "            #     k_repeats = 0\n",
    "\n",
    "            # if k_repeats > 2:\n",
    "            #     print(\"converged!!\")\n",
    "            #     return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 1.2480930098810668\n",
      "epoch 2 0.16601747991950103\n",
      "epoch 3 0.09399524419852992\n",
      "epoch 4 0.07181912906889298\n",
      "epoch 5 0.06096475562185019\n",
      "epoch 6 0.05444664585790814\n",
      "epoch 7 0.049872758603950025\n",
      "epoch 8 0.046602157939669965\n",
      "epoch 9 0.04415451451126207\n",
      "epoch 10 0.04202747502025329\n",
      "epoch 11 0.04038629142855408\n",
      "epoch 12 0.03899186531686826\n",
      "epoch 13 0.0378164943150461\n",
      "epoch 14 0.03671045051623328\n",
      "epoch 15 0.03585942189879667\n"
     ]
    }
   ],
   "source": [
    "theta_opt = gradient_descent(X_train, y_train, 100, 0.5, 1e-5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.03496685118310934"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "total_cost(theta_opt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0.9646666666666667\n",
      "test 0.8897\n"
     ]
    }
   ],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)\n",
    "\n",
    "m = X_train.shape[0]\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "\n",
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1\n",
    "\n",
    "acc = 0\n",
    "for i in range(m):\n",
    "    o_pred = forward_propagation(X_train[i], theta_opt)\n",
    "    acc += int(np.argmax(o_pred[-1]) == y_train[i])\n",
    "\n",
    "print(\"train\", acc/m)\n",
    "\n",
    "acc = 0\n",
    "for i in range(m_test):\n",
    "    o_pred = forward_propagation(X_test[i], theta_opt)\n",
    "    acc += int(np.argmax(o_pred[-1]) == y_test[i])\n",
    "\n",
    "print(\"test\", acc/m_test)"
   ]
  }
 ]
}