{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:15000]\n",
    "y_train = y_train[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    ohe = np.zeros((y.size, y.max()+1))\n",
    "    ohe[np.arange(y.size), y] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1 # batch size\n",
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "hidden_layers = [10]\n",
    "r = 10\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "# X_train = X_train / 255 # scale to 0-1\n",
    "y_train = one_hot_encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "indices = np.arange(m)\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = hidden_layers.copy()\n",
    "layers.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "        sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netj(theta_j, x_j):\n",
    "    return np.dot(theta_j.T, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    o = [np.zeros(l) for l in layers]\n",
    "\n",
    "    for l in range(len(layers)):\n",
    "        for j in range(layers[l]):\n",
    "            if l == 0:\n",
    "                o[l][j] = g(netj(theta[l][j], np.hstack((np.ones(1), x))))\n",
    "            else:\n",
    "                o[l][j] = g(netj(theta[l][j], np.hstack((np.ones(1), o[l-1]))))\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, o, theta):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            y: class labels\n",
    "            o: outputs of each layer\n",
    "            theta: parameters\n",
    "        Returns:\n",
    "            deltas: deltas[l][j] for each layer l and perceptron j\n",
    "    \"\"\"\n",
    "    deltas = [np.zeros(l) for l in layers]\n",
    "\n",
    "    # output layer\n",
    "    output_layer = -1\n",
    "    delta = (y - o[output_layer]) * o[output_layer] * (1 - o[output_layer])\n",
    "    deltas[output_layer] = delta\n",
    "\n",
    "    # hidden layers\n",
    "    for l in reversed(range(len(hidden_layers))):\n",
    "        for j in range(hidden_layers[l]):\n",
    "            deltas[l][j] = sum(deltas[l+1][dwn_nbr] * theta[l+1][dwn_nbr, j] * o[l][j] * (1 - o[l][j]) for dwn_nbr in range(layers[l+1]))\n",
    "        \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(theta, X, Y):\n",
    "    m = X.shape[0]\n",
    "    error = 0\n",
    "    for i in range(m):\n",
    "        x, y = X[i], Y[i]\n",
    "        o = forward_propagation(x, theta)\n",
    "        error += np.sum((y - o[-1]) ** 2)\n",
    "    return error / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.4177357821552805"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "total_cost(theta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, M, learning_rate, epsilon, max_epochs):\n",
    "    \"\"\"\n",
    "        mini-batch SGD\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    t = 0\n",
    "    k_repeats = 0\n",
    "    theta = [np.random.rand(layers[0], n+1)] + [np.random.rand(layers[l], layers[l-1]+1) for l in range(1, len(layers))]\n",
    "    # prev_theta = copy.deepcopy(theta)\n",
    "    # prev_cost = -1\n",
    "    prev_cost = total_cost(theta, X_train, y_train)\n",
    "\n",
    "    # theta = [np.zeros((layers[0], n+1))] + [np.zeros((layers[l], layers[l-1]+1)) for l in range(1, len(layers))]\n",
    "    # print(\"theta\", theta)\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        if epoch > max_epochs:\n",
    "            return theta\n",
    "\n",
    "        print(\"epoch\", epoch, total_cost(theta, X_train, y_train))\n",
    "\n",
    "        # shuffle\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_e = X_train[indices]\n",
    "        y_train_e = y_train[indices]\n",
    "\n",
    "        for b in range(int(m/M)):\n",
    "            t += 1\n",
    "            # print(\"b\", b, epoch, t)\n",
    "            sum_J_theta_derivatives = [np.zeros((layers[0], n+1))] + [np.zeros((layers[l], layers[l-1]+1)) for l in range(1, len(layers))]\n",
    "\n",
    "            for i in range(b * M, (b+1) * M):\n",
    "                x, y = X_train_e[i], y_train_e[i]\n",
    "                o = forward_propagation(x, theta)\n",
    "                deltas = back_propagation(y, o, theta)\n",
    "\n",
    "                # print(\"c\", y, o[-1], y - o[-1])\n",
    "                # cost += np.sum((y - o[-1]) ** 2)\n",
    "                # print(\"x\", x.shape, x)\n",
    "                # print(\"o\", o)\n",
    "                # print(\"deltas\", deltas)\n",
    "\n",
    "                # calculate J(theta) derivatives\n",
    "                for l in range(len(layers)):\n",
    "                    if l == 0:\n",
    "                        x_j = np.hstack((np.ones(1), x))\n",
    "                    else:\n",
    "                        x_j = np.hstack((np.ones(1), o[l-1]))\n",
    "                    for j in range(layers[l]):\n",
    "                        J_theta_derivative = - deltas[l][j] * x_j\n",
    "                        sum_J_theta_derivatives[l][j] += J_theta_derivative\n",
    "\n",
    "            # prev_theta = copy.deepcopy(theta)\n",
    "            # print(\"sumJ\", sum_J_theta_derivatives)\n",
    "            # print(\"err\", total_cost(theta, X_train, y_train))\n",
    "\n",
    "            # update theta\n",
    "            for l in range(len(layers)):\n",
    "                for j in range(layers[l]):\n",
    "                    theta[l][j] = theta[l][j] - learning_rate * (sum_J_theta_derivatives[l][j] / M)\n",
    "\n",
    "            cost = total_cost(theta, X_train, y_train)\n",
    "            # print(\"cost\", prev_cost, cost, abs(prev_cost - cost))\n",
    "            if abs(prev_cost - cost) <= epsilon:\n",
    "                k_repeats += 1\n",
    "            else:\n",
    "                k_repeats = 0\n",
    "\n",
    "            if k_repeats > 1:\n",
    "                print(\"converged\")\n",
    "                return theta\n",
    "            prev_cost = cost\n",
    "\n",
    "            # print(\"w\", theta)\n",
    "\n",
    "            # break when stopping criteria meets\n",
    "            # converged = True\n",
    "            # theta_diff_all = 0\n",
    "            # for l in range(len(layers)):\n",
    "            #     # print(\"theta\", theta[l], prev_theta[l])\n",
    "            #     # print(\"theta_diff\", abs(theta[l] - prev_theta[l]))\n",
    "            #     theta_diff_all += np.sum(abs(theta[l] - prev_theta[l]))\n",
    "            #     if not (abs(theta[l] - prev_theta[l]) <= epsilon).all():\n",
    "            #         converged = False\n",
    "            #         break\n",
    "            # # print(\"stop\", coverged, theta_diff_all)\n",
    "            # if converged:\n",
    "            #     k_repeats += 1\n",
    "            # else:\n",
    "            #     k_repeats = 0\n",
    "\n",
    "            # if k_repeats > 2:\n",
    "            #     print(\"converged!!\")\n",
    "            #     return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 4.433368953545143\n",
      "epoch 2 0.84473238719306\n",
      "epoch 3 0.45008193093226584\n",
      "epoch 4 0.4500479464755232\n",
      "converged\n"
     ]
    }
   ],
   "source": [
    "theta_opt = gradient_descent(X_train, y_train, 100, 0.5, 1e-6, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4500666523720425"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "total_cost(theta_opt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": [
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1\n",
    "y_test = one_hot_encode(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[0.86991288, 0.32321739, 0.83377991, ..., 0.82080913, 0.07864687,\n",
       "         0.35536597],\n",
       "        [0.7883652 , 0.19921964, 0.96623474, ..., 0.38366965, 0.5132148 ,\n",
       "         0.38671487],\n",
       "        [0.43544278, 0.20834028, 0.73324651, ..., 0.11509007, 0.03154932,\n",
       "         0.67572304],\n",
       "        ...,\n",
       "        [0.08513006, 0.41104814, 0.51339887, ..., 0.89236081, 0.8689392 ,\n",
       "         0.7074392 ],\n",
       "        [0.99745042, 0.05653553, 0.45763151, ..., 0.42595098, 0.15559207,\n",
       "         0.27000851],\n",
       "        [0.82311161, 0.05872331, 0.72825314, ..., 0.70812267, 0.65214924,\n",
       "         0.53505052]]),\n",
       " array([[ 0.17288016, -0.01834767,  0.05160397, -0.27903105, -0.63207329,\n",
       "         -0.46659137,  0.16931675, -0.172058  , -0.37038378,  0.0104089 ,\n",
       "         -0.58379329],\n",
       "        [-0.19192866,  0.11521075,  0.0814054 ,  0.12284444, -0.56885953,\n",
       "         -0.30784747, -0.10268008, -0.33978089, -0.49283215, -0.64688701,\n",
       "          0.13491257],\n",
       "        [ 0.14332961, -0.20412825, -0.00567166, -0.64390761, -0.34932877,\n",
       "          0.03841263, -0.29528403,  0.10131842, -0.49929299, -0.32815148,\n",
       "         -0.14931364],\n",
       "        [ 0.09979498, -0.41486595,  0.10280464,  0.13017208, -0.39653308,\n",
       "         -0.76960535, -0.4555081 ,  0.08899542, -0.04467822, -0.61516559,\n",
       "         -0.00079099],\n",
       "        [ 0.17556624, -0.03588964,  0.18859147, -0.06244975,  0.05541925,\n",
       "         -0.60391784,  0.15540478, -0.51601706, -0.66680369, -0.20727208,\n",
       "         -0.64209021],\n",
       "        [ 0.26558383,  0.14183315, -0.429315  , -0.55477914, -0.11351914,\n",
       "         -0.59076634, -0.34253944, -0.05048985, -0.62233269,  0.26893444,\n",
       "         -0.11593758],\n",
       "        [-0.60740512,  0.08596891, -0.36730359,  0.17649685, -0.24672153,\n",
       "          0.28205792, -0.24935619, -0.06564323, -0.24028438, -0.43434743,\n",
       "         -0.55036829],\n",
       "        [-0.10341343, -0.11671294, -0.16869921, -0.01032607, -0.18264216,\n",
       "          0.1403155 , -0.10716883, -0.7170625 , -0.37977414, -0.39240987,\n",
       "         -0.18318008],\n",
       "        [ 0.02766852, -0.14279847, -0.0741806 , -0.47098297, -0.25341409,\n",
       "          0.09542296, -0.64150679,  0.17449553, -0.12151415, -0.58691513,\n",
       "         -0.19737877],\n",
       "        [-0.13732604, -0.07843995, -0.14819512, -0.43804629, -0.34384052,\n",
       "          0.06823165,  0.07123812, -0.30830568,  0.01523491, -0.37508506,\n",
       "         -0.55000128]])]"
      ]
     },
     "metadata": {},
     "execution_count": 344
    }
   ],
   "source": [
    "theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.10523046 0.09570368 0.10130242 0.10370809 0.10464016 0.09657548\n 0.10038259 0.09881762 0.09730301 0.09452589] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n[-0.10523046 -0.09570368 -0.10130242 -0.10370809 -0.10464016 -0.09657548\n -0.10038259  0.90118238 -0.09730301 -0.09452589] [0.01107345 0.00915919 0.01026218 0.01075537 0.01094956 0.00932682\n 0.01007666 0.81212968 0.00946787 0.00893514] 0.9021359403644441\n"
     ]
    }
   ],
   "source": [
    "tt = 4357\n",
    "\n",
    "o_pred = forward_propagation(X_train[tt], theta_opt)\n",
    "\n",
    "print(o_pred[-1], y_train[tt])\n",
    "# o_pred[-1]\n",
    "\n",
    "dd = y_train[tt] - o_pred[-1]\n",
    "\n",
    "# print(y_train[230], o_pred[-1])\n",
    "\n",
    "print(dd, dd ** 2, np.sum(dd ** 2))"
   ]
  }
 ]
}