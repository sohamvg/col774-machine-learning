{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    ohe = np.zeros((y.size, y.max()+1))\n",
    "    ohe[np.arange(y.size), y] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "hidden_layers = [1]\n",
    "r = 10\n",
    "layers = hidden_layers.copy()\n",
    "layers.append(r)\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "y_train = one_hot_encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    return 1 * (z > 0) # 1 for z[i] > 0 else 0 for each i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(theta, x):\n",
    "    \"\"\"\n",
    "        Returns net_j = sum(theta_j . x_j)\n",
    "    \"\"\"\n",
    "    return theta.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta, only_output_layer, use_relu):\n",
    "    \"\"\"\n",
    "        Arguments: \n",
    "            x: input example\n",
    "            theta: parmeters\n",
    "            only_output_layer: if True, return only output of last layer else return output for all layers\n",
    "        Returns:\n",
    "            o: outputs of each layer l and neuron j by forward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    o = [np.zeros(l) for l in layers]\n",
    "\n",
    "    if use_relu:\n",
    "        g = ReLU\n",
    "    else:\n",
    "        g = sigmoid\n",
    "\n",
    "    for l in range(len(layers)):\n",
    "        if l == 0: # first layer: input is x from training data\n",
    "            o[l] = g(net(theta[l], x))\n",
    "        elif l == len(layers) - 1: # output layer: use sigmoid always\n",
    "            o[l] = sigmoid(net(theta[l], o[l-1]))\n",
    "        else: # hidden layers: input is output of prev layer\n",
    "            o[l] = g(net(theta[l], o[l-1]))\n",
    "\n",
    "    if only_output_layer:\n",
    "        return o[-1]\n",
    "    else:\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, o, theta, use_relu):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            y: class labels\n",
    "            o: outputs of each layer\n",
    "            theta: parameters\n",
    "        Returns:\n",
    "            deltas: deltas[l][j] for each layer l and neuron j by backpropagation\n",
    "    \"\"\"\n",
    "    deltas = [np.zeros(l) for l in layers]\n",
    "\n",
    "    # output layer\n",
    "    output_layer = -1\n",
    "    delta = (y - o[output_layer]) * o[output_layer] * (1 - o[output_layer])\n",
    "    deltas[output_layer] = delta\n",
    "\n",
    "    # hidden layers\n",
    "    for l in reversed(range(len(hidden_layers))):\n",
    "        if use_relu:\n",
    "            derivative = ReLU_derivative(o[l])\n",
    "        else:\n",
    "            derivative = o[l] * (1 - o[l]) # equivalent to derivative of sigmoid(netj)\n",
    "\n",
    "        deltas[l] = (theta[l+1].T @ deltas[l+1]) * derivative # = sum(deltas[l+1][dwn_nbr] * theta[l+1][dwn_nbr, j] * derivative for dwn_nbr in range(layers[l+1]))\n",
    "        \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(theta, X, Y, use_relu):\n",
    "    m = X.shape[0]\n",
    "    outputs = np.apply_along_axis(forward_propagation2, 1, X, theta, True, use_relu)\n",
    "    return np.sum((outputs - Y) ** 2) / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(n, layers):\n",
    "    # He initialization\n",
    "    theta = [np.random.randn(layers[0], n) * np.sqrt(2/(n))] + [np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/layers[l-1]) for l in range(1, len(layers))]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, M, learning_rate, epsilon, max_epochs, adaptive_learning, use_relu):\n",
    "    \"\"\"\n",
    "        mini-batch SGD\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    k_repeats = 0\n",
    "    k_repeats_limit = 2\n",
    "    theta = init_theta(n, layers)\n",
    "    prev_cost = np.inf\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        if epoch > max_epochs:\n",
    "            return theta\n",
    "\n",
    "        if adaptive_learning:\n",
    "            learning_rate = 0.5 / np.sqrt(epoch)\n",
    "\n",
    "        print(\"epoch\", epoch, get_cost(theta, X_train, y_train, use_relu), learning_rate, time.time() - t0)\n",
    "\n",
    "        # shuffle at each epoch\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_e = X_train[indices]\n",
    "        y_train_e = y_train[indices]\n",
    "\n",
    "        for b in range(int(m/M)):\n",
    "            sum_J_theta_derivatives = [np.zeros((layers[0], n))] + [np.zeros((layers[l], layers[l-1])) for l in range(1, len(layers))]\n",
    "\n",
    "            for i in range(b * M, (b+1) * M):\n",
    "                x, y = X_train_e[i], y_train_e[i]\n",
    "                o = forward_propagation2(x, theta, only_output_layer=False, use_relu=use_relu)\n",
    "                deltas = back_propagation2(y, o, theta, use_relu)\n",
    "\n",
    "                # calculate J(theta) derivatives\n",
    "                for l in range(len(layers)):\n",
    "                    if l == 0:\n",
    "                        x_j = x\n",
    "                    else:\n",
    "                        x_j = o[l-1]\n",
    "                    for j in range(layers[l]):\n",
    "                        J_theta_derivative = - deltas[l][j] * x_j\n",
    "                        sum_J_theta_derivatives[l][j] += J_theta_derivative # sum over J(theta) derivatives over the batch\n",
    "\n",
    "            # calculating cost over the examples seen in the lastest batch before updating theta\n",
    "            cost = get_cost(theta, X_train_e[b * M: (b+1) * M], y_train_e[b * M: (b+1) * M], use_relu)\n",
    "            if abs(prev_cost - cost) <= epsilon:\n",
    "                k_repeats += 1\n",
    "            else:\n",
    "                k_repeats = 0\n",
    "\n",
    "            if k_repeats >= k_repeats_limit:\n",
    "                print(\"converged\")\n",
    "                return theta\n",
    "            prev_cost = cost\n",
    "\n",
    "            # update theta\n",
    "            for l in range(len(layers)):\n",
    "                    theta[l] = theta[l] - learning_rate * (sum_J_theta_derivatives[l] / M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 1.251868796491821 0.2 0.7239954471588135\n",
      "epoch 2 0.09626699136686201 0.2 7.4801716804504395\n",
      "epoch 3 0.06708203804556807 0.2 14.044877052307129\n",
      "epoch 4 0.05719477509986197 0.2 20.333900928497314\n",
      "epoch 5 0.05154458581180813 0.2 26.603447437286377\n",
      "epoch 6 0.04790138041116837 0.2 32.909809589385986\n",
      "epoch 7 0.045337861077174586 0.2 39.25278639793396\n",
      "epoch 8 0.04325231084283853 0.2 46.45357155799866\n",
      "epoch 9 0.041800947637161696 0.2 53.55356407165527\n",
      "epoch 10 0.04016288265537348 0.2 60.78118085861206\n",
      "epoch 11 0.038850821389720194 0.2 70.68149065971375\n",
      "epoch 12 0.03792558625924073 0.2 77.76149034500122\n",
      "epoch 13 0.036788570797042786 0.2 84.7254958152771\n",
      "epoch 14 0.03583285780447631 0.2 91.55201292037964\n",
      "epoch 15 0.03519570124087757 0.2 98.39052367210388\n",
      "epoch 16 0.034315934810453745 0.2 105.34255409240723\n",
      "epoch 17 0.033888507299502095 0.2 112.38655519485474\n",
      "epoch 18 0.033023811449058214 0.2 119.33883595466614\n",
      "epoch 19 0.03254031252073965 0.2 126.2627854347229\n",
      "epoch 20 0.03195626573900832 0.2 133.10381984710693\n",
      "epoch 21 0.031486330761981894 0.2 139.8798644542694\n",
      "epoch 22 0.030979853600398306 0.2 146.54384875297546\n",
      "epoch 23 0.030618987416191436 0.2 153.30747866630554\n",
      "epoch 24 0.030193538949256873 0.2 160.19943714141846\n",
      "epoch 25 0.02971170580116891 0.2 167.13129329681396\n",
      "epoch 26 0.029371552221704945 0.2 174.0233268737793\n",
      "epoch 27 0.029082817758800845 0.2 180.93688011169434\n",
      "epoch 28 0.02879500995838148 0.2 187.87964057922363\n",
      "epoch 29 0.028458409107117458 0.2 194.77970266342163\n",
      "epoch 30 0.028361388309712056 0.2 201.61566185951233\n",
      "epoch 31 0.028300390252400883 0.2 208.5824635028839\n",
      "epoch 32 0.027541158049070178 0.2 215.5386679172516\n",
      "epoch 33 0.02740036812036152 0.2 223.0946888923645\n",
      "epoch 34 0.027158143301441635 0.2 230.08725666999817\n",
      "epoch 35 0.02681191883725157 0.2 237.08326172828674\n",
      "epoch 36 0.026715011435182238 0.2 244.01128220558167\n",
      "epoch 37 0.02662067422255953 0.2 251.08332514762878\n",
      "epoch 38 0.026296496523365787 0.2 257.9853127002716\n",
      "epoch 39 0.026030008018301347 0.2 264.9654121398926\n",
      "epoch 40 0.025841317320036045 0.2 272.0013747215271\n",
      "epoch 41 0.025573696854401726 0.2 279.0450699329376\n",
      "epoch 42 0.02542495529290403 0.2 285.9818217754364\n",
      "epoch 43 0.025225186129383872 0.2 292.953782081604\n",
      "epoch 44 0.02503490236199176 0.2 299.9938097000122\n",
      "epoch 45 0.024883512981909568 0.2 306.95388555526733\n",
      "epoch 46 0.02477207243037946 0.2 315.52536177635193\n",
      "epoch 47 0.024510747144750076 0.2 322.8013594150543\n",
      "epoch 48 0.0244106016961325 0.2 329.7856526374817\n",
      "epoch 49 0.024256918883691744 0.2 336.97832322120667\n",
      "epoch 50 0.024083221124192932 0.2 343.8066761493683\n",
      "epoch 51 0.02396098207135115 0.2 350.6878592967987\n",
      "epoch 52 0.023806160976687116 0.2 357.6358554363251\n",
      "epoch 53 0.02366974630274744 0.2 365.06781029701233\n",
      "epoch 54 0.023570790975750853 0.2 372.09403347969055\n",
      "epoch 55 0.023636988041691624 0.2 378.8940315246582\n",
      "epoch 56 0.023355653611123545 0.2 385.78400802612305\n",
      "epoch 57 0.02315654089160398 0.2 392.7020208835602\n",
      "epoch 58 0.023067132683638546 0.2 399.73797941207886\n",
      "epoch 59 0.022970780037427627 0.2 406.68601393699646\n",
      "epoch 60 0.02289764149425379 0.2 413.5020241737366\n",
      "epoch 61 0.022744050179388273 0.2 420.3179795742035\n",
      "epoch 62 0.02260255660577574 0.2 427.14201641082764\n",
      "epoch 63 0.022649239209055098 0.2 433.89002799987793\n",
      "epoch 64 0.02238062150368438 0.2 440.69471526145935\n",
      "epoch 65 0.022439364356522547 0.2 447.84132170677185\n",
      "epoch 66 0.02229090744220279 0.2 454.7604055404663\n",
      "epoch 67 0.022109904427957346 0.2 462.07016468048096\n",
      "epoch 68 0.022351652023781234 0.2 468.9381585121155\n",
      "epoch 69 0.021982900673400363 0.2 475.7461590766907\n",
      "epoch 70 0.021783528934264693 0.2 482.61955070495605\n",
      "epoch 71 0.021762139778539077 0.2 489.54751801490784\n",
      "epoch 72 0.02172542147285272 0.2 496.480637550354\n",
      "epoch 73 0.021626995124211894 0.2 503.50712966918945\n",
      "epoch 74 0.021485429949239677 0.2 510.43110275268555\n",
      "epoch 75 0.021488782165986247 0.2 517.7870953083038\n",
      "epoch 76 0.02131476021895971 0.2 525.355094909668\n",
      "epoch 77 0.021286311439736902 0.2 532.6631000041962\n",
      "epoch 78 0.02120389943166754 0.2 539.9494936466217\n",
      "epoch 79 0.021087306893048822 0.2 547.3050022125244\n",
      "epoch 80 0.021011628616169214 0.2 554.6449391841888\n",
      "epoch 81 0.02097183779612982 0.2 562.0409789085388\n",
      "epoch 82 0.02093188127672437 0.2 569.349800825119\n",
      "epoch 83 0.021058389002074376 0.2 576.7618706226349\n",
      "epoch 84 0.02077222572636899 0.2 584.1388375759125\n",
      "epoch 85 0.020765409913203862 0.2 591.4789054393768\n",
      "epoch 86 0.020742121475287423 0.2 598.8284928798676\n",
      "epoch 87 0.020626431088228897 0.2 606.2365310192108\n",
      "epoch 88 0.020675012918358084 0.2 614.0502851009369\n",
      "epoch 89 0.02050097428531489 0.2 621.7783629894257\n",
      "epoch 90 0.02046880283963765 0.2 629.2142851352692\n",
      "epoch 91 0.020272129830802153 0.2 636.5235772132874\n",
      "epoch 92 0.020234449438838016 0.2 643.9796221256256\n",
      "epoch 93 0.02030090980119439 0.2 651.365583896637\n",
      "epoch 94 0.020114891903150366 0.2 658.7295849323273\n",
      "epoch 95 0.020193873300296823 0.2 666.0496599674225\n",
      "epoch 96 0.01996429707428407 0.2 673.405788898468\n",
      "epoch 97 0.019965579932048675 0.2 680.7519683837891\n",
      "epoch 98 0.019885027790483126 0.2 688.1630034446716\n",
      "epoch 99 0.019773027046570708 0.2 695.8807623386383\n",
      "epoch 100 0.019697804403709624 0.2 703.2790722846985\n",
      "done 709.8831419944763\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "theta_opt = gradient_descent(X_train, y_train, M=100, learning_rate=0.5, epsilon=1e-4, max_epochs=100, adaptive_learning=False, use_relu=True)\n",
    "\n",
    "print(\"done\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.019688736245311384"
      ]
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "get_cost(theta_opt, X_train, y_train, use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X, use_relu):\n",
    "    outputs = np.apply_along_axis(forward_propagation2, 1, X, theta, True, use_relu)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0.9801333333333333\ntest 0.9108\n"
     ]
    }
   ],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)\n",
    "\n",
    "m = X_train.shape[0]\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "\n",
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1\n",
    "\n",
    "\n",
    "acc = 0\n",
    "# for i in range(m):\n",
    "#     o_pred = forward_propagation(X_train[i], theta_opt, use_relu=True)\n",
    "#     acc += int(np.argmax(o_pred[-1]) == y_train[i])\n",
    "acc = np.sum(predict(theta_opt, X_train, use_relu=True) == y_train)\n",
    "\n",
    "print(\"train\", acc/m)\n",
    "\n",
    "acc = 0\n",
    "# for i in range(m_test):\n",
    "#     o_pred = forward_propagation(X_test[i], theta_opt, use_relu=True)\n",
    "#     acc += int(np.argmax(o_pred[-1]) == y_test[i])\n",
    "\n",
    "acc = np.sum(predict(theta_opt, X_test, use_relu=True) == y_test)\n",
    "\n",
    "print(\"test\", acc/m_test)"
   ]
  }
 ]
}