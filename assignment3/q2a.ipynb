{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:15000]\n",
    "y_train = y_train[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    ohe = np.zeros((y.size, y.max()+1))\n",
    "    ohe[np.arange(y.size), y] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "hidden_layers = [15]\n",
    "r = 10\n",
    "layers = hidden_layers.copy()\n",
    "layers.append(r)\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "# y_train = one_hot_encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "        sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netj(theta_j, x_j):\n",
    "    return np.dot(theta_j.T, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    o = [np.zeros(l) for l in layers]\n",
    "\n",
    "    for l in range(len(layers)):\n",
    "        for j in range(layers[l]):\n",
    "            if l == 0:\n",
    "                o[l][j] = g(netj(theta[l][j], x))\n",
    "            else:\n",
    "                o[l][j] = g(netj(theta[l][j], o[l-1])) # use all outputs of prev layer as network is fully connected\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, o, theta):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            y: class labels\n",
    "            o: outputs of each layer\n",
    "            theta: parameters\n",
    "        Returns:\n",
    "            deltas: deltas[l][j] for each layer l and perceptron j\n",
    "    \"\"\"\n",
    "    deltas = [np.zeros(l) for l in layers]\n",
    "\n",
    "    # output layer\n",
    "    output_layer = -1\n",
    "    delta = (y - o[output_layer]) * o[output_layer] * (1 - o[output_layer])\n",
    "    deltas[output_layer] = delta\n",
    "\n",
    "    # hidden layers\n",
    "    for l in reversed(range(len(hidden_layers))):\n",
    "        for j in range(hidden_layers[l]):\n",
    "            deltas[l][j] = sum(deltas[l+1][dwn_nbr] * theta[l+1][dwn_nbr, j] * o[l][j] * (1 - o[l][j]) for dwn_nbr in range(layers[l+1]))\n",
    "        \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(theta, X, Y):\n",
    "    m = X.shape[0]\n",
    "    error = 0\n",
    "    for i in range(m):\n",
    "        x, y = X[i], Y[i]\n",
    "        o = forward_propagation(x, theta)\n",
    "        error += np.sum((y - o[-1]) ** 2)\n",
    "    return error / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n0.552697555151562\n[array([0.63476122, 0.55350336, 0.69467035, 0.53026362, 0.5342025 ,\n       0.43449275, 0.5095823 , 0.3942845 , 0.56823072, 0.51860935,\n       0.39584177, 0.51310929, 0.7072353 , 0.42276449, 0.46413004]), array([0.5627912 , 0.678119  , 0.71507255, 0.4537618 , 0.64135179,\n       0.51538003, 0.30011677, 0.6488654 , 0.31167408, 0.45500852])]\n0.2524978000853511\n6\n0.03797362268914106\n[array([0.50949227, 0.41121447, 0.44858935, 0.56697234, 0.58073581,\n       0.50376129, 0.4745573 , 0.48521124, 0.62170819, 0.53858699,\n       0.38319595, 0.58885346, 0.5632412 , 0.47311895, 0.65047057]), array([0.54220668, 0.67135202, 0.71278775, 0.486648  , 0.62616902,\n       0.58630912, 0.30626527, 0.65315984, 0.34641246, 0.43501986])]\n0.16922945601884032\n456\n0.37219187702359735\n[array([0.59198851, 0.44221212, 0.52080524, 0.55532562, 0.58720703,\n       0.44337361, 0.57781215, 0.51273734, 0.41421409, 0.42528272,\n       0.51040702, 0.51141345, 0.43262057, 0.54335195, 0.50748039]), array([0.5032613 , 0.64567614, 0.68425885, 0.47922814, 0.60586629,\n       0.53905589, 0.30736028, 0.65343981, 0.35589615, 0.37991399])]\n0.013045388691834386\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.4391922195860023"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "theta0 = [np.random.randn(layers[0], n) * np.sqrt(2/(n))] + [np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/layers[l-1]) for l in range(1, len(layers))]\n",
    "# theta0 = [np.random.randn(layers[0], n) * 0.001] + [np.random.randn(layers[l], layers[l-1]) * 0.001 for l in range(1, len(layers))]\n",
    "# theta0 = [np.zeros((layers[0], n+1))] + [np.zeros((layers[l], layers[l-1]+1)) for l in range(1, len(layers))]\n",
    "# theta0 = [np.random.rand(layers[0], n+1)] + [np.random.rand(layers[l], layers[l-1]+1) for l in range(1, len(layers))]\n",
    "\n",
    "\n",
    "# print(theta0[0][0])\n",
    "for ind in [5, 6, 456]:\n",
    "    xt = X_train[ind]\n",
    "    print(ind)\n",
    "    # print(xt)\n",
    "    print(netj(theta0[0][0], xt))\n",
    "    out0 = forward_propagation(xt, theta0)\n",
    "    print(out0)\n",
    "    print(netj(theta0[1][0], out0[0]))\n",
    "total_cost(theta0, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, M, learning_rate, epsilon, max_epochs):\n",
    "    \"\"\"\n",
    "        mini-batch SGD\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    t = 0\n",
    "    k_repeats = 0\n",
    "    # theta = [np.random.rand(layers[0], n+1)] + [np.random.rand(layers[l], layers[l-1]+1) for l in range(1, len(layers))]\n",
    "    theta = [np.random.randn(layers[0], n) * np.sqrt(2/(n))] + [np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/layers[l-1]) for l in range(1, len(layers))]\n",
    "    # prev_theta = copy.deepcopy(theta)\n",
    "    # prev_cost = -1\n",
    "    # prev_cost = total_cost(theta, X_train, y_train)\n",
    "\n",
    "    # theta = [np.zeros((layers[0], n+1))] + [np.zeros((layers[l], layers[l-1]+1)) for l in range(1, len(layers))]\n",
    "    # print(\"theta\", theta)\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        if epoch > max_epochs:\n",
    "            return theta\n",
    "\n",
    "        print(\"epoch\", epoch, total_cost(theta, X_train, y_train))\n",
    "\n",
    "        # shuffle\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_e = X_train[indices]\n",
    "        y_train_e = y_train[indices]\n",
    "\n",
    "        for b in range(int(m/M)):\n",
    "            t += 1\n",
    "            # print(\"b\", b, epoch, t)\n",
    "            sum_J_theta_derivatives = [np.zeros((layers[0], n))] + [np.zeros((layers[l], layers[l-1])) for l in range(1, len(layers))]\n",
    "\n",
    "            for i in range(b * M, (b+1) * M):\n",
    "                x, y = X_train_e[i], y_train_e[i]\n",
    "                o = forward_propagation(x, theta)\n",
    "                deltas = back_propagation(y, o, theta)\n",
    "\n",
    "                # print(\"c\", y, o[-1], y - o[-1])\n",
    "                # cost += np.sum((y - o[-1]) ** 2)\n",
    "                # print(\"x\", x.shape, x)\n",
    "                # print(\"o\", o)\n",
    "                # print(\"deltas\", deltas)\n",
    "\n",
    "                # calculate J(theta) derivatives\n",
    "                for l in range(len(layers)):\n",
    "                    if l == 0:\n",
    "                        # x_j = np.hstack((np.ones(1), x))\n",
    "                        x_j = x\n",
    "                    else:\n",
    "                        # x_j = np.hstack((np.ones(1), o[l-1]))\n",
    "                        x_j = o[l-1]\n",
    "                    for j in range(layers[l]):\n",
    "                        J_theta_derivative = - deltas[l][j] * x_j\n",
    "                        sum_J_theta_derivatives[l][j] += J_theta_derivative\n",
    "\n",
    "            # prev_theta = copy.deepcopy(theta)\n",
    "            # print(\"sumJ\", sum_J_theta_derivatives)\n",
    "            # print(\"err\", total_cost(theta, X_train, y_train))\n",
    "\n",
    "            # update theta\n",
    "            for l in range(len(layers)):\n",
    "                for j in range(layers[l]):\n",
    "                    theta[l][j] = theta[l][j] - learning_rate * (sum_J_theta_derivatives[l][j] / M)\n",
    "\n",
    "            # cost = total_cost(theta, X_train, y_train)\n",
    "            # # print(\"cost\", prev_cost, cost, abs(prev_cost - cost))\n",
    "            # if abs(prev_cost - cost) <= epsilon:\n",
    "            #     k_repeats += 1\n",
    "            # else:\n",
    "            #     k_repeats = 0\n",
    "\n",
    "            # if k_repeats > 1:\n",
    "            #     print(\"converged\")\n",
    "            #     return theta\n",
    "            # prev_cost = cost\n",
    "\n",
    "            # print(\"w\", theta)\n",
    "\n",
    "            # break when stopping criteria meets\n",
    "            # converged = True\n",
    "            # theta_diff_all = 0\n",
    "            # for l in range(len(layers)):\n",
    "            #     # print(\"theta\", theta[l], prev_theta[l])\n",
    "            #     # print(\"theta_diff\", abs(theta[l] - prev_theta[l]))\n",
    "            #     theta_diff_all += np.sum(abs(theta[l] - prev_theta[l]))\n",
    "            #     if not (abs(theta[l] - prev_theta[l]) <= epsilon).all():\n",
    "            #         converged = False\n",
    "            #         break\n",
    "            # # print(\"stop\", coverged, theta_diff_all)\n",
    "            # if converged:\n",
    "            #     k_repeats += 1\n",
    "            # else:\n",
    "            #     k_repeats = 0\n",
    "\n",
    "            # if k_repeats > 2:\n",
    "            #     print(\"converged!!\")\n",
    "            #     return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 1.2954430856441295\n",
      "epoch 2 0.381024452313904\n",
      "epoch 3 0.28697542670549486\n",
      "epoch 4 0.21277853216797626\n",
      "epoch 5 0.16651281976017654\n"
     ]
    }
   ],
   "source": [
    "theta_opt = gradient_descent(X_train, y_train, 100, 0.5, 1e-6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.13789600276230932"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "total_cost(theta_opt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1\n",
    "# y_test = one_hot_encode(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[ 0.09464465, -0.02737903,  0.01860194, ...,  0.03251293,\n",
       "          0.02473725,  0.02264009],\n",
       "        [-0.01868205, -0.12385706,  0.03245024, ...,  0.0730092 ,\n",
       "         -0.06262206,  0.0559998 ],\n",
       "        [-0.02562323,  0.06347721,  0.00437212, ...,  0.04151092,\n",
       "          0.06693867,  0.05158981],\n",
       "        ...,\n",
       "        [-0.03669108,  0.00560304,  0.00119363, ..., -0.06088707,\n",
       "          0.0116376 ,  0.03520283],\n",
       "        [ 0.05112716, -0.00102293,  0.04138122, ...,  0.01205604,\n",
       "          0.02369106,  0.02701591],\n",
       "        [ 0.01359898, -0.04127114, -0.04589829, ..., -0.00516434,\n",
       "          0.02983482, -0.04506464]]),\n",
       " array([[-2.56477056e-01, -1.61752914e-01, -1.31269219e+00,\n",
       "         -5.58403040e-01, -1.06648291e+00, -8.48061974e-01,\n",
       "          9.79730537e-01, -2.39385691e-01,  5.09355037e-01,\n",
       "          4.20678246e-01, -1.32694100e-01, -7.89876914e-01,\n",
       "         -1.97356853e+00,  3.44453002e-01, -1.26664578e-01],\n",
       "        [-1.14423167e+00, -1.25474696e+00, -5.28335652e-01,\n",
       "          5.36599151e-02, -2.27000521e-01,  1.19269013e+00,\n",
       "         -2.80422538e-01, -7.85639989e-01, -1.58236604e+00,\n",
       "          9.10424719e-01,  1.02309318e+00, -8.12033326e-02,\n",
       "         -1.11079816e+00,  1.93148363e-01, -1.49483175e-01],\n",
       "        [-1.10248786e+00,  2.64533241e-02,  1.52986495e+00,\n",
       "         -7.62660840e-01, -2.00313752e+00, -3.34189472e-01,\n",
       "         -7.86799021e-02, -8.73560262e-01, -1.06820554e-01,\n",
       "         -1.25822208e+00,  4.78465089e-01, -6.88973505e-02,\n",
       "          7.31129280e-01,  2.28663532e-01, -1.10433373e+00],\n",
       "        [-8.77283662e-01, -1.18834723e-01, -4.97360636e-01,\n",
       "         -1.12473431e+00,  6.44936503e-01, -8.53041136e-03,\n",
       "         -7.41133812e-01,  6.72384236e-01,  3.51331176e-01,\n",
       "          3.28531603e-01, -4.58492680e-01, -1.49104364e+00,\n",
       "          5.77957932e-01,  8.74900499e-02, -2.43690008e+00],\n",
       "        [ 1.07914513e+00, -1.67709780e-01, -1.33510718e+00,\n",
       "         -7.73147214e-01,  3.20693384e-01,  7.76062589e-02,\n",
       "         -1.30113979e+00,  8.97586945e-01, -1.27141284e-01,\n",
       "         -2.24573428e+00,  1.35772961e-01,  5.84710042e-01,\n",
       "          8.39476317e-02, -5.52201365e-01, -8.98114185e-01],\n",
       "        [ 3.45132989e-01, -1.92068960e+00,  1.05056491e+00,\n",
       "         -8.79950886e-02,  8.82547435e-01,  9.06912656e-02,\n",
       "         -3.54250937e-02, -3.98448677e-01, -1.40140650e+00,\n",
       "         -1.70666035e+00, -6.86268598e-01, -7.63330587e-01,\n",
       "          4.27260154e-01, -3.58945538e-02, -4.62347754e-02],\n",
       "        [-8.39395491e-01,  3.08755949e-01, -5.09995726e-01,\n",
       "          1.65311200e+00, -4.67784436e-01,  2.87858659e-01,\n",
       "         -1.41138983e+00, -9.00553095e-01,  1.35529341e-01,\n",
       "         -3.41859901e-01,  6.93345681e-01, -8.14625181e-01,\n",
       "         -1.34779577e-03, -6.47605417e-01, -6.19950564e-01],\n",
       "        [ 9.38354025e-01, -2.70220603e-01, -1.30791341e+00,\n",
       "          4.85444412e-01, -2.74233731e-01, -6.17744942e-01,\n",
       "         -1.77908248e+00, -5.46591687e-01,  4.70492941e-01,\n",
       "          1.02462638e+00, -6.88505132e-01, -1.37598628e+00,\n",
       "         -1.41834874e-01, -5.28198550e-01, -3.95596622e-01],\n",
       "        [ 7.18294378e-01, -1.47319955e+00,  6.49429094e-01,\n",
       "          4.60490103e-01, -1.46058039e+00,  8.55825776e-01,\n",
       "         -4.79417304e-01,  9.10492953e-01,  2.85011910e-01,\n",
       "         -9.79923881e-01, -4.72829516e-01,  2.74120015e-01,\n",
       "         -1.93712926e+00, -7.78035094e-01,  2.51122909e-02],\n",
       "        [ 1.60892473e-01,  5.86235185e-01, -3.02736778e-02,\n",
       "         -1.00460302e+00,  7.25011297e-01, -9.59050927e-02,\n",
       "         -4.82145928e-01, -1.00589168e+00, -1.55256913e+00,\n",
       "          6.60940465e-01, -4.96640249e-01,  1.16197767e-01,\n",
       "         -2.12473672e-01, -2.07258981e+00,  2.98456305e-01]])]"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 2\n[-0.03333155 -0.04925013  0.24252155 -0.08327136 -0.0731166  -0.24075464\n -0.05264166 -0.00671038 -0.0227034  -0.10264247] [1.11099215e-03 2.42557493e-03 5.88167022e-02 6.93411868e-03\n 5.34603706e-03 5.79627984e-02 2.77114478e-03 4.50292486e-05\n 5.15444484e-04 1.05354764e-02] 0.14646331835006932\n"
     ]
    }
   ],
   "source": [
    "tt = 432\n",
    "\n",
    "o_pred = forward_propagation(X_test[tt], theta_opt)\n",
    "\n",
    "# print(o_pred[-1], y_train[tt])\n",
    "# o_pred[-1]\n",
    "print(np.argmax(o_pred[-1]), y_test[tt])\n",
    "\n",
    "dd = y_train[tt] - o_pred[-1]\n",
    "\n",
    "# print(y_train[230], o_pred[-1])\n",
    "\n",
    "print(dd, dd ** 2, np.sum(dd ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8969\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(m):\n",
    "    o_pred = forward_propagation(X_train[i], theta_opt)\n",
    "    acc += int(np.argmax(o_pred[-1]) == y_train[i])\n",
    "\n",
    "print(acc/m)"
   ]
  }
 ]
}