{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3b97f6205e13cf85ea01e4081d87b6f06bf272a6e460f325999b991fb6c68282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    ohe = np.zeros((y.size, y.max()+1))\n",
    "    ohe[np.arange(y.size), y] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "n = 28 * 28\n",
    "hidden_layers = [1]\n",
    "r = 10\n",
    "layers = hidden_layers.copy()\n",
    "layers.append(r)\n",
    "\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "y_train = one_hot_encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return max(0.0, z)\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netj(theta_j, x_j):\n",
    "    return np.dot(theta_j.T, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta, use_relu):\n",
    "    \"\"\"\n",
    "        Arguments: \n",
    "            x: input example\n",
    "            theta: parmeters\n",
    "        Returns:\n",
    "            o: outputs of each layer l and neuron j by forward propagation\n",
    "    \"\"\"\n",
    "    o = [np.zeros(l) for l in layers]\n",
    "\n",
    "    if use_relu:\n",
    "        g = ReLU\n",
    "    else:\n",
    "        g = sigmoid\n",
    "\n",
    "    for l in range(len(layers)):\n",
    "        for j in range(layers[l]):\n",
    "            if l == 0: # first layer: input is x from training data\n",
    "                o[l][j] = g(netj(theta[l][j], x))\n",
    "            elif l == len(layers) - 1: # output layer: use sigmoid always\n",
    "                o[l][j] = sigmoid(netj(theta[l][j], o[l-1]))\n",
    "            else: # hidden layers: input is output of prev layer\n",
    "                o[l][j] = g(netj(theta[l][j], o[l-1])) # use all outputs of prev layer as network is fully connected\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, o, theta, use_relu):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            y: class labels\n",
    "            o: outputs of each layer\n",
    "            theta: parameters\n",
    "        Returns:\n",
    "            deltas: deltas[l][j] for each layer l and neuron j by backpropagation\n",
    "    \"\"\"\n",
    "    deltas = [np.zeros(l) for l in layers]\n",
    "\n",
    "    # output layer\n",
    "    output_layer = -1\n",
    "    delta = (y - o[output_layer]) * o[output_layer] * (1 - o[output_layer])\n",
    "    deltas[output_layer] = delta\n",
    "\n",
    "    # hidden layers\n",
    "    for l in reversed(range(len(hidden_layers))):\n",
    "        for j in range(hidden_layers[l]):\n",
    "            if use_relu:\n",
    "                derivative = ReLU_derivative(o[l][j])\n",
    "            else:\n",
    "                derivative = o[l][j] * (1 - o[l][j])\n",
    "            deltas[l][j] = sum(deltas[l+1][dwn_nbr] * theta[l+1][dwn_nbr, j] * derivative for dwn_nbr in range(layers[l+1]))\n",
    "        \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(theta, X, Y, use_relu):\n",
    "    m = X.shape[0]\n",
    "    error = 0\n",
    "    for i in range(m):\n",
    "        x, y = X[i], Y[i]\n",
    "        o = forward_propagation(x, theta, use_relu)\n",
    "        error += np.sum((y - o[-1]) ** 2)\n",
    "    return error / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(n, layers):\n",
    "    # He initialization\n",
    "    theta = [np.random.randn(layers[0], n) * np.sqrt(2/(n))] + [np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/layers[l-1]) for l in range(1, len(layers))]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, M, learning_rate, epsilon, max_epochs, adaptive_learning, use_relu):\n",
    "    \"\"\"\n",
    "        mini-batch SGD\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    k_repeats = 0\n",
    "    k_repeats_limit = 2\n",
    "    theta = init_theta(n, layers)\n",
    "    prev_cost = np.inf\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        if epoch > max_epochs:\n",
    "            return theta\n",
    "\n",
    "        if adaptive_learning:\n",
    "            learning_rate = 0.5 / np.sqrt(epoch)\n",
    "\n",
    "        print(\"epoch\", epoch, total_cost(theta, X_train, y_train, use_relu), learning_rate, time.time() - t0)\n",
    "\n",
    "        # shuffle at each epoch\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_e = X_train[indices]\n",
    "        y_train_e = y_train[indices]\n",
    "\n",
    "        for b in range(int(m/M)):\n",
    "            sum_J_theta_derivatives = [np.zeros((layers[0], n))] + [np.zeros((layers[l], layers[l-1])) for l in range(1, len(layers))]\n",
    "\n",
    "            for i in range(b * M, (b+1) * M):\n",
    "                x, y = X_train_e[i], y_train_e[i]\n",
    "                o = forward_propagation(x, theta, use_relu)\n",
    "                deltas = back_propagation(y, o, theta, use_relu)\n",
    "\n",
    "                # calculate J(theta) derivatives\n",
    "                for l in range(len(layers)):\n",
    "                    if l == 0:\n",
    "                        x_j = x\n",
    "                    else:\n",
    "                        x_j = o[l-1]\n",
    "                    for j in range(layers[l]):\n",
    "                        J_theta_derivative = - deltas[l][j] * x_j\n",
    "                        sum_J_theta_derivatives[l][j] += J_theta_derivative # sum over J(theta) derivatives over the batch\n",
    "\n",
    "            # calculating cost over the examples seen in the lastest batch before updating theta\n",
    "            cost = total_cost(theta, X_train_e[b * M: (b+1) * M], y_train_e[b * M: (b+1) * M], use_relu)\n",
    "            if abs(prev_cost - cost) <= epsilon:\n",
    "                k_repeats += 1\n",
    "            else:\n",
    "                k_repeats = 0\n",
    "\n",
    "            if k_repeats >= k_repeats_limit:\n",
    "                print(\"converged\")\n",
    "                return theta\n",
    "            prev_cost = cost\n",
    "\n",
    "            # update theta\n",
    "            for l in range(len(layers)):\n",
    "                    theta[l] = theta[l] - learning_rate * (sum_J_theta_derivatives[l] / M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 1.166222756685179 1 3.049055337905884\n",
      "converged\n",
      "done 8.114614725112915\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "theta_opt = gradient_descent(X_train, y_train, M=100, learning_rate=1, epsilon=5e-5, max_epochs=60, adaptive_learning=False, use_relu=False)\n",
    "\n",
    "print(\"done\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.450200498870224"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "total_cost(theta_opt, X_train, y_train, use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 0.1\n",
      "test 0.1\n"
     ]
    }
   ],
   "source": [
    "X_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_train.npy\"\n",
    "y_train_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_train.npy\"\n",
    "X_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/X_test.npy\"\n",
    "y_test_file = \"C:/IITD/sem5/col774-ml/datasets/kannada_digits/neural_network_kannada/y_test.npy\"\n",
    "\n",
    "X_train, y_train = np.load(X_train_file), np.load(y_train_file)\n",
    "X_test, y_test = np.load(X_test_file), np.load(y_test_file)\n",
    "\n",
    "m = X_train.shape[0]\n",
    "X_train = X_train.reshape((m, n)) # reshape\n",
    "X_train = X_train / 255 # scale to 0-1\n",
    "\n",
    "m_test = X_test.shape[0]\n",
    "X_test = X_test.reshape((m_test, n)) # reshape\n",
    "X_test = X_test / 255 # scale to 0-1\n",
    "\n",
    "acc = 0\n",
    "for i in range(m):\n",
    "    o_pred = forward_propagation(X_train[i], theta_opt, use_relu=True)\n",
    "    acc += int(np.argmax(o_pred[-1]) == y_train[i])\n",
    "\n",
    "print(\"train\", acc/m)\n",
    "\n",
    "acc = 0\n",
    "for i in range(m_test):\n",
    "    o_pred = forward_propagation(X_test[i], theta_opt, use_relu=True)\n",
    "    acc += int(np.argmax(o_pred[-1]) == y_test[i])\n",
    "\n",
    "print(\"test\", acc/m_test)"
   ]
  }
 ]
}